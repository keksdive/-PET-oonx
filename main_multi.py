import numpy as np
import os
import json
import tensorflow as tf
from multi_agent import MultiAgentManager
from reward_utils import calculate_hybrid_reward
from visualization import visualize_spectral_curves

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
DATA_DIR = r"D:\Processed_Result\67w-38w\procession-data"
NUM_BANDS_TO_SELECT = 30
TOTAL_EPISODES = 400  # å¤šæ™ºèƒ½ä½“é€šå¸¸æ”¶æ•›æ›´å¿«

# åˆ’åˆ†ä¸‰ä¸ªæ™ºèƒ½ä½“çš„è´Ÿè´£åŒºåŸŸ (æ ¹æ® 208 ä¸ªæ³¢æ®µå¹³å‡åˆ’åˆ†)
# VNIR (0-69), SWIR1 (70-139), SWIR2 (140-208)
AGENT_RANGES = [(0, 70), (70, 140), (140, 208)]
# ===============================================

gpus = tf.config.list_physical_devices('GPU')
if gpus: tf.config.experimental.set_memory_growth(gpus[0], True)


def load_data():
    X = np.load(os.path.join(DATA_DIR, "X.npy"), mmap_mode='r')
    y = np.load(os.path.join(DATA_DIR, "y.npy"))
    return X, y


def prepare_balanced_data(X_full, y_full, n_per_class=2000):
    # (ä¿æŒåŸæœ‰çš„å¹³è¡¡é‡‡æ ·é€»è¾‘ï¼Œæ­¤å¤„çœç•¥ä»¥èŠ‚çœç¯‡å¹…ï¼Œç›´æ¥è°ƒç”¨å³å¯)
    # è¯·å¤ç”¨æ‚¨ä¹‹å‰ main.py ä¸­çš„ prepare_multiclass_drl_data å‡½æ•°
    indices = []
    for cls in np.unique(y_full):
        idx = np.where(y_full == cls)[0]
        if len(idx) > 0:
            indices.append(np.random.choice(idx, min(len(idx), n_per_class), replace=False))
    indices = np.concatenate(indices)
    np.random.shuffle(indices)
    return X_full[indices].astype(np.float32), y_full[indices]


def train_multi_agent():
    # 1. æ•°æ®å‡†å¤‡
    X_full, y_full = load_data()
    X_drl, y_drl = prepare_balanced_data(X_full, y_full, n_per_class=1500)
    total_bands = X_full.shape[1]

    # 2. åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
    manager = MultiAgentManager(total_bands, AGENT_RANGES)
    print(f"\nğŸš€ å¯åŠ¨å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿ ({len(manager.agents)} Agents)...")

    best_reward = -float('inf')
    best_bands = []

    for e in range(TOTAL_EPISODES):
        state = np.zeros(total_bands)
        selected_bands = []
        episode_reward = 0

        for step in range(NUM_BANDS_TO_SELECT):
            # A. ååŒå†³ç­–ï¼šManager è¯¢é—®æ‰€æœ‰ Agent å¹¶é€‰å‡ºæœ€ä½³
            action = manager.get_global_action(state, selected_bands)

            # B. è®¡ç®—æ··åˆå¥–åŠ± (MI + Correlation)
            reward = calculate_hybrid_reward(selected_bands, action, X_drl, y_drl, alpha=2.5, beta=1.0)

            # C. çŠ¶æ€æ›´æ–°
            next_state = state.copy()
            next_state[action] = 1
            done = (len(selected_bands) == NUM_BANDS_TO_SELECT - 1)

            # D. å­˜å‚¨ç»éªŒ (è‡ªåŠ¨åˆ†å‘ç»™å¯¹åº”çš„ Agent)
            manager.remember(state, action, reward, next_state, done)

            # E. è®­ç»ƒæ‰€æœ‰ Agent
            manager.train()

            state = next_state
            selected_bands.append(action)
            episode_reward += reward

        # F. åŒæ­¥ Target ç½‘ç»œ & è¡°å‡æ¢ç´¢ç‡
        manager.update_targets()
        manager.decay_epsilon()

        # G. è®°å½•æœ€ä½³ç»“æœ
        if episode_reward > best_reward:
            best_reward = episode_reward
            best_bands = sorted(selected_bands)
            print(f"ğŸŒŸ [New Best] Ep {e + 1} | Reward: {episode_reward:.4f} | Bands: {best_bands}")

        if (e + 1) % 10 == 0:
            print(f"Ep {e + 1}/{TOTAL_EPISODES} | Reward: {episode_reward:.2f} | Epsilon: {manager.epsilon:.3f}")

    return best_bands


if __name__ == "__main__":
    final_bands = train_multi_agent()

    # ä¿å­˜ç»“æœ
    with open("best_bands_multi_agent.json", "w") as f:
        json.dump({"selected_bands": [int(b) for b in final_bands]}, f)

    print(f"\nâœ… æœ€ç»ˆé€‰æ‹©æ³¢æ®µ: {final_bands}")

    # å¯è§†åŒ–
    print("ğŸ“Š ç”Ÿæˆå¤šæ™ºèƒ½ä½“é€‰æ‹©åˆ†å¸ƒå›¾...")
    X_full, y_full = load_data()
    X_plot, y_plot = prepare_balanced_data(X_full, y_full, n_per_class=500)
    visualize_spectral_curves(X_plot, y_plot, final_bands, "Fig_MultiAgent_Selection.png")