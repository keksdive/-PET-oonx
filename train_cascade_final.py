import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
import tf2onnx
import datetime

# ================= 1. å…¨å±€é…ç½® =================
# [è¯·ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…æ•°æ®è·¯å¾„]
DATA_DIR = r"D:\Processed_Result\material-feature"
JSON_PATH = r"D:\Processed_Result\json-procession-result\material_specific_features.json"

# è¾“å‡ºè·¯å¾„
MODEL_SAVE_DIR = r"D:\Processed_Result\final_cascade_model"
RESULT_DIR = r"D:\Processed_Result\final_cascade_results"

if not os.path.exists(MODEL_SAVE_DIR): os.makedirs(MODEL_SAVE_DIR)
if not os.path.exists(RESULT_DIR): os.makedirs(RESULT_DIR)

# æ˜¾å­˜é…ç½®
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        from tensorflow.keras import mixed_precision

        mixed_precision.set_global_policy('mixed_float16')
        print("âš¡ æ··åˆç²¾åº¦åŠ é€Ÿå·²å¼€å¯")
    except:
        pass

# ç±»åˆ«å®šä¹‰
NUM_CLASSES = 4
CLASS_NAMES = ["Background", "PET", "PA", "CC"]  # ID: 0, 1, 2, 3


# ================= 2. æ•°æ®é›†æˆåˆ†æ ¸æŸ¥å·¥å…· =================
def print_dataset_composition(y, name="Dataset"):
    """
    æ‰“å°æ•°æ®é›†ä¸­å„ç±»åˆ«çš„å…·ä½“æ•°é‡ï¼Œç¡®ä¿æ˜¯æ··åˆæ•°æ®é›†
    """
    unique, counts = np.unique(y, return_counts=True)
    count_dict = dict(zip(unique, counts))

    print(f"\nğŸ“Š [{name}] æˆåˆ†åˆ†æ:")
    total = len(y)
    for cls_idx, count in count_dict.items():
        # æ˜ å°„ ID åˆ°åç§°
        idx = int(cls_idx)
        cls_name = CLASS_NAMES[idx] if idx < len(CLASS_NAMES) else f"Class {idx}"
        percent = (count / total) * 100
        print(f"   - {cls_name:<10}: {count:>6} æ ·æœ¬ ({percent:.2f}%)")

    # éªŒè¯æ˜¯å¦ç¼ºç±» (æˆ‘ä»¬åªå…³å¿ƒ 1, 2, 3ï¼ŒèƒŒæ™¯0å·²è¢«è¿‡æ»¤)
    required = {1, 2, 3}
    present = set(unique.astype(int))
    if not required.issubset(present):
        print(f"   âš ï¸ è­¦å‘Š: è¯¥æ•°æ®é›†ä¸­ç¼ºå¤±éƒ¨åˆ†ç›®æ ‡ç±»åˆ«ï¼ç°æœ‰: {present}")
    else:
        print(f"   âœ… éªŒè¯é€šè¿‡: åŒ…å«æ‰€æœ‰ç›®æ ‡æè´¨(PET/PA/CC)ï¼Œæ˜¯æ··åˆæ•°æ®é›†ã€‚")


# ================= 3. æ•°æ®å¤„ç† =================

def load_and_preprocess_data():
    print("ğŸ“¥ åŠ è½½æ•°æ®...")
    X = np.load(os.path.join(DATA_DIR, "X.npy")).astype(np.float32)
    y = np.load(os.path.join(DATA_DIR, "y.npy")).astype(np.float32)

    # 1. è‡ªåŠ¨è¡¥å…¨å¯¼æ•°
    if X.shape[1] == 208:
        print("âš ï¸ è‡ªåŠ¨è®¡ç®—å¯¼æ•°ç‰¹å¾...")
        X_deriv = np.gradient(X, axis=1)
        X = np.concatenate([X, X_deriv], axis=1)

    # 2. è¯»å–æ³¢æ®µé…ç½®
    if os.path.exists(JSON_PATH):
        with open(JSON_PATH, 'r') as f:
            config = json.load(f)
        selected_bands = set()
        for mat in config['materials'].values():
            selected_bands.update(mat['selected_bands'])
        selected_bands = sorted(list(selected_bands))
        X = X[:, selected_bands]
        print(f"ğŸ”ª ç‰¹å¾åˆ‡ç‰‡å®Œæˆ: {X.shape} (ä½¿ç”¨ {len(selected_bands)} ä¸ªæ³¢æ®µ)")
    else:
        print("âš ï¸ æœªæ‰¾åˆ° JSONï¼Œä½¿ç”¨å…¨æ³¢æ®µ")

    # 3. è¿‡æ»¤èƒŒæ™¯ (ID 0)
    # æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŒºåˆ† PET, PA, CCã€‚èƒŒæ™¯ç”±é¢„å¤„ç†é˜ˆå€¼å¤„ç†ã€‚
    valid_mask = y != 0
    X = X[valid_mask]
    y = y[valid_mask]

    print(f"ğŸ§¹ è¿‡æ»¤èƒŒæ™¯åæ ·æœ¬æ•°: {len(y)}")

    # 4. æ„é€ å¤šå¤´æ ‡ç­¾ (Multi-Head Labels)
    # y_pet: æ˜¯PET=1, å…¶ä»–=0
    # y_pa:  æ˜¯PA=1,  å…¶ä»–=0
    # CC å¯¹åº”: y_pet=0 ä¸” y_pa=0
    y_pet = np.where(y == 1, 1.0, 0.0).astype(np.float32)
    y_pa = np.where(y == 2, 1.0, 0.0).astype(np.float32)

    return X, y, y_pet, y_pa


# ================= 4. æ¨¡å‹ç»„ä»¶ =================

class SpectralAugment(layers.Layer):
    """å¼ºæ•°æ®å¢å¼ºï¼šæŠ—åç§»ã€æŠ—å™ªå£°"""

    def __init__(self, shift_range=5, scale_range=0.3, noise_std=0.05, **kwargs):
        super().__init__(**kwargs)
        self.shift = shift_range
        self.scale = scale_range
        self.noise_std = noise_std

    def call(self, inputs, training=True):
        if not training: return inputs
        batch_size = tf.shape(inputs)[0]
        shift = tf.random.uniform([batch_size], minval=-self.shift, maxval=self.shift + 1, dtype=tf.int32)
        x = tf.map_fn(lambda args: tf.roll(args[0], shift=args[1], axis=0), (inputs, shift),
                      fn_output_signature=inputs.dtype)
        gain = tf.random.uniform([batch_size, 1], minval=1.0 - self.scale, maxval=1.0 + self.scale, dtype=inputs.dtype)
        x = x * gain
        noise = tf.random.normal(tf.shape(x), stddev=self.noise_std, dtype=inputs.dtype)
        return x + noise

    def get_config(self):
        config = super().get_config()
        config.update({"shift_range": self.shift, "scale_range": self.scale, "noise_std": self.noise_std})
        return config


class CascadeLogicLayer(layers.Layer):
    """
    [æ ¸å¿ƒé€»è¾‘å±‚] In-Graph Logic
    é€»è¾‘ï¼šFirst Check PET -> Then Check PA -> Else CC
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, inputs):
        # inputs[0]: pet_prob, inputs[1]: pa_prob
        pet_prob, pa_prob = inputs

        is_pet = tf.greater(pet_prob, 0.5)
        is_pa = tf.greater(pa_prob, 0.5)

        # è¾“å‡ºåŸå§‹ Label ID: 1.0(PET), 2.0(PA), 3.0(CC)
        final_id = tf.where(is_pet, 1.0, tf.where(is_pa, 2.0, 3.0))
        return final_id


# ================= 5. æ¨¡å‹æ„å»º =================

def build_multi_head_model(input_shape):
    inputs = layers.Input(shape=input_shape, name="spectral_input")

    # å¢å¼º
    x = SpectralAugment()(inputs)
    x = layers.Reshape((input_shape[0], 1))(x)

    # ä¸»å¹² (Backbone)
    x = layers.Conv1D(32, 5, padding="same", activation="relu")(x)
    x = layers.MaxPooling1D(2)(x)
    x = layers.Conv1D(64, 3, padding="same", activation="relu")(x)
    x = layers.MaxPooling1D(2)(x)

    attn = layers.MultiHeadAttention(num_heads=2, key_dim=32)(x, x)
    x = layers.Add()([x, attn])
    x = layers.LayerNormalization()(x)

    features = layers.GlobalAveragePooling1D()(x)
    features = layers.Dropout(0.5)(features)

    # å¤´ A: PET åˆ¤æ–­
    x_pet = layers.Dense(32, activation="relu")(features)
    out_pet = layers.Dense(1, activation="sigmoid", name="head_pet")(x_pet)

    # å¤´ B: PA åˆ¤æ–­
    x_pa = layers.Dense(32, activation="relu")(features)
    out_pa = layers.Dense(1, activation="sigmoid", name="head_pa")(x_pa)

    # é€»è¾‘å±‚ (ç”¨äºæ¨ç†/ONNX)
    final_id = CascadeLogicLayer(name="final_logic")([out_pet, out_pa])

    return models.Model(inputs=inputs, outputs=[out_pet, out_pa, final_id])


# ================= 6. éªŒè¯ç›‘æ§å›è°ƒ =================

class LogicMetrics(callbacks.Callback):
    def __init__(self, val_data):
        super().__init__()
        self.X_val, self.y_val_true = val_data

    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % 5 == 0:
            preds = self.model.predict(self.X_val, verbose=0)
            y_pred_id = preds[2].flatten()  # final_logic è¾“å‡º

            # ç›‘æ§ PET å¬å›ç‡
            pet_mask = (self.y_val_true == 1)
            pet_acc = np.mean(y_pred_id[pet_mask] == 1) if np.sum(pet_mask) > 0 else 0

            # ç›‘æ§ CC çº¯åº¦ (æ˜¯å¦è¢«è¯¯åˆ¤ä¸º PA)
            cc_mask = (self.y_val_true == 3)
            cc_as_pa = np.mean(y_pred_id[cc_mask] == 2) if np.sum(cc_mask) > 0 else 0

            print(f"\nğŸ§ [éªŒè¯é›†ç›‘æ§] PETå¬å›ç‡: {pet_acc:.4f} | CCè¢«è¯¯åˆ¤ä¸ºPAç‡: {cc_as_pa:.4f} (è¶Šä½è¶Šå¥½)")


# ================= 7. ä¸»æµç¨‹ =================

if __name__ == "__main__":
    # 1. å‡†å¤‡æ•°æ®
    X, y_raw, y_pet, y_pa = load_and_preprocess_data()
    indices = np.arange(len(X))

    # ================= ğŸš¨ æ··åˆæ•°æ®é›†åˆ’åˆ† (60/20/20) ğŸš¨ =================
    print(f"\nğŸ“¦ æ­£åœ¨è¿›è¡Œæ··åˆæ•°æ®é›†åˆ’åˆ† (Stratified Split)...")

    # ç¬¬ä¸€æ¬¡åˆ‡åˆ†ï¼šç•™å‡º 20% ä½œä¸ºæœ€ç»ˆæµ‹è¯•é›† (Test)
    X_tv, X_test, y_tv, y_test, idx_tv, idx_test = train_test_split(
        X, y_raw, indices, test_size=0.2, stratify=y_raw, random_state=42
    )

    # ç¬¬äºŒæ¬¡åˆ‡åˆ†ï¼šä»å‰©ä½™ 80% ä¸­ç•™å‡º 25% ä½œä¸ºéªŒè¯é›† (Val) -> ç›¸å½“äºæ€»ä½“çš„ 20%
    X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(
        X_tv, y_tv, idx_tv, test_size=0.25, stratify=y_tv, random_state=42
    )

    # åŒæ­¥åˆ‡åˆ†å¤šå¤´æ ‡ç­¾
    y_pet_train = y_pet[idx_train]
    y_pet_val = y_pet[idx_val]
    y_pet_test = y_pet[idx_test]

    y_pa_train = y_pa[idx_train]
    y_pa_val = y_pa[idx_val]
    y_pa_test = y_pa[idx_test]

    # ================= ğŸ” æ ¸å¿ƒï¼šéªŒè¯é›†æˆåˆ†æ ¸æŸ¥ ğŸ” =================
    print_dataset_composition(y_train, "Train Set (è®­ç»ƒé›†)")
    print_dataset_composition(y_val, "Val Set (éªŒè¯é›†)")
    print_dataset_composition(y_test, "Test Set (æµ‹è¯•é›†)")
    # ==========================================================

    # 2. å‡†å¤‡è®­ç»ƒæ•°æ®å­—å…¸
    train_inputs = X_train
    train_outputs = {
        "head_pet": y_pet_train,
        "head_pa": y_pa_train,
        "final_logic": y_train  # å ä½ï¼Œloss weightä¸º0
    }

    val_inputs = X_val
    val_outputs = {
        "head_pet": y_pet_val,
        "head_pa": y_pa_val,
        "final_logic": y_val
    }

    # 3. æ„å»ºæ¨¡å‹
    model = build_multi_head_model((X.shape[1],))

    model.compile(
        optimizer=optimizers.Adam(1e-4),
        loss={
            "head_pet": "binary_crossentropy",
            "head_pa": "binary_crossentropy",
            "final_logic": None
        },
        # æƒé‡ç­–ç•¥ï¼šPETè¯†åˆ«æœ€é‡è¦(1.0)ï¼Œå…¶æ¬¡æ˜¯PA(0.5)ï¼ŒCCé æ’é™¤
        loss_weights={
            "head_pet": 1.0,
            "head_pa": 0.5,
            "final_logic": 0.0
        },
        metrics={"head_pet": "accuracy", "head_pa": "accuracy"}
    )

    # 4. è®­ç»ƒ
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒçº§è”é€»è¾‘æ¨¡å‹...")
    history = model.fit(
        train_inputs, train_outputs,
        validation_data=(val_inputs, val_outputs),
        epochs=100,
        batch_size=256,
        callbacks=[
            callbacks.EarlyStopping(patience=15, restore_best_weights=True),
            LogicMetrics((val_inputs, y_val))  # è‡ªå®šä¹‰ç›‘æ§
        ]
    )

    # 5. æœ€ç»ˆè¯„ä¼° (åŸºäº Test é›†)
    print("\nğŸ“Š æœ€ç»ˆè¯„ä¼° (Test Set)...")
    raw_preds = model.predict(X_test)
    final_pred_ids = raw_preds[2].flatten()  # å–é€»è¾‘å±‚è¾“å‡º

    print(classification_report(y_test, final_pred_ids, target_names=["PET", "PA", "CC"]))

    cm = confusion_matrix(y_test, final_pred_ids)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["PET", "PA", "CC"], yticklabels=["PET", "PA", "CC"])
    plt.title("Final Cascade Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.savefig(os.path.join(RESULT_DIR, "cascade_confusion.png"))

    # 6. ä¿å­˜åŒæ ¼å¼
    # A. H5
    h5_path = os.path.join(MODEL_SAVE_DIR, "cascade_model.h5")
    model.save(h5_path)
    print(f"âœ… H5 æ¨¡å‹å·²ä¿å­˜: {h5_path}")

    # B. ONNX (ç›´æ¥è¾“å‡º Label ID)
    onnx_path = os.path.join(MODEL_SAVE_DIR, "cascade_model.onnx")
    spec = (tf.TensorSpec((None, X.shape[1]), tf.float32, name="spectral_input"),)

    try:
        model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)
        with open(onnx_path, "wb") as f:
            f.write(model_proto.SerializeToString())
        print(f"âœ… ONNX æ¨¡å‹å·²å¯¼å‡º: {onnx_path}")
        print("ğŸ’¡ ONNX è¾“å‡º: [pet_prob, pa_prob, final_label_id]")
    except Exception as e:
        print(f"âŒ ONNX å¯¼å‡ºå¤±è´¥: {e}")