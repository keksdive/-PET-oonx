import os
import numpy as np
import spectral.io.envi as envi
import glob
import json
import cv2
import random

# ================= âš™ï¸ å¤šæºæ•°æ®é›†é…ç½®åŒºåŸŸ =================
DATASETS = [
    # 1. PET æ–‡ä»¶å¤¹ (æ­£æ ·æœ¬)
    {
        "spe_dir": r"D:\Train_Data\fake_img\train-PET",
        "json_dir": r"D:\Train_Data\fake_img\train-PET\fake_images"
    },
    # 2. é PET æ–‡ä»¶å¤¹ (CC) -> å°†ä½œä¸ºè´Ÿæ ·æœ¬ (èƒŒæ™¯/å®¹æ˜“åŒºåˆ†)
    {
        "spe_dir": r"D:\Train_Data\no_PET\CC",
        "json_dir": None
    },
    # 3. é PET æ–‡ä»¶å¤¹ (PA) -> å°†ä½œä¸ºå›°éš¾è´Ÿæ ·æœ¬ (éœ€é‡ç‚¹åŠ æƒ)
    {
        "spe_dir": r"D:\Train_Data\no_PET\PA",
        "json_dir": None
    }
]

# [æ–°å¢] æ ¡å‡†æ–‡ä»¶è·¯å¾„
WHITE_REF_PATH = r"D:\Train_Data\DWA\white_ref.spe"
DARK_REF_PATH = r"D:\Train_Data\DWA\dark_ref.spe"

# è¾“å‡ºä¿å­˜è·¯å¾„
OUTPUT_DIR = r"D:\Processed_Result\67w-38w\procession-data"

# æ ‡ç­¾å®šä¹‰ï¼šä¸‰åˆ†ç±»é€»è¾‘ (æ”¯æŒå›°éš¾æ ·æœ¬æŒ–æ˜)
LABEL_MAP = {
    "PET": 1,       # æ­£æ ·æœ¬
    "NON_PET": 0,   # æ™®é€šè´Ÿæ ·æœ¬ (èƒŒæ™¯, CC, PPç­‰)
    "PA": 2         # å›°éš¾è´Ÿæ ·æœ¬ (å°¼é¾™) -> å¯¹åº” Class Weight é«˜æƒé‡
}

# é‡‡æ ·å‚æ•°
SAMPLES_PER_IMAGE = 4000
THRESHOLD_RATIO = 0.05
TARGET_BANDS = 208  # å¼ºåˆ¶å¯¹é½æ³¢æ®µæ•°


# =======================================================

def repair_hdr_file(hdr_path):
    """è‡ªåŠ¨ä¿®å¤ç¼ºå°‘çš„ byte order"""
    try:
        with open(hdr_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        content = "".join(lines).lower()
        if "byte order" not in content:
            lines.append("\nbyte order = 0\n")
            with open(hdr_path, 'w', encoding='utf-8') as f:
                f.writelines(lines)
    except:
        pass


def load_calibration_data(white_path, dark_path):
    """åŠ è½½é»‘ç™½æ ¡å‡†æ–‡ä»¶å¹¶è®¡ç®—å¹³å‡å…‰è°±"""
    print(f"âšª åŠ è½½ç™½æ¿: {white_path}")
    print(f"âš« åŠ è½½é»‘æ¿: {dark_path}")

    def load_mean(path):
        hdr = os.path.splitext(path)[0] + ".hdr"
        repair_hdr_file(hdr)
        if not os.path.exists(path) or not os.path.exists(hdr):
            raise FileNotFoundError(f"ç¼ºå¤±æ ¡å‡†æ–‡ä»¶: {path}")
        img = envi.open(hdr, path).load()
        return np.mean(img, axis=(0, 1)).astype(np.float32)

    try:
        w = load_mean(white_path)
        d = load_mean(dark_path)
        return w, d
    except Exception as e:
        print(f"âŒ æ ¡å‡†æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        exit()


def load_envi_image_with_calibration(hdr_path, white_ref, dark_ref):
    """åŠ è½½ ENVI å›¾åƒå¹¶ç«‹å³æ‰§è¡Œé»‘ç™½æ ¡æ­£"""
    try:
        repair_hdr_file(hdr_path)
        base = os.path.splitext(hdr_path)[0]
        spe_path = base + ".spe"
        if not os.path.exists(spe_path): spe_path = base + ".raw"
        if not os.path.exists(spe_path): return None

        # 1. åŠ è½½åŸå§‹ RAW æ•°æ®
        img_obj = envi.open(hdr_path, spe_path)
        raw_data = np.array(img_obj.load(), dtype=np.float32)

        # 2. ç»´åº¦ä¿®æ­£
        shape = raw_data.shape
        if shape[1] < shape[2] and shape[1] in [206, 208, 224]:
            raw_data = np.transpose(raw_data, (0, 2, 1))

        H, W, B = raw_data.shape

        # 3. é»‘ç™½æ ¡æ­£
        if white_ref.shape[0] != B:
            w_res = cv2.resize(white_ref.reshape(1, -1), (B, 1)).flatten()
            d_res = cv2.resize(dark_ref.reshape(1, -1), (B, 1)).flatten()
        else:
            w_res, d_res = white_ref, dark_ref

        denom = w_res - d_res
        denom[denom == 0] = 1e-6
        reflectance = (raw_data - d_res) / denom

        # 4. æ³¢æ®µå¯¹é½
        if TARGET_BANDS is not None and B != TARGET_BANDS:
            flat = reflectance.reshape(-1, B)
            flat_resized = cv2.resize(flat, (TARGET_BANDS, H * W), interpolation=cv2.INTER_LINEAR)
            reflectance = flat_resized.reshape(H, W, TARGET_BANDS)

        return reflectance

    except Exception as e:
        print(f"âŒ åŠ è½½æˆ–æ ¡æ­£å¤±è´¥ {os.path.basename(hdr_path)}: {e}")
        return None


def get_mask_from_json(json_path, image_shape):
    """ä¼˜å…ˆä½¿ç”¨ JSON æ ‡æ³¨"""
    H, W = image_shape[:2]
    mask = np.zeros((H, W), dtype=np.uint8)
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        for shape in data.get('shapes', []):
            points = np.array(shape['points'], dtype=np.int32)
            cv2.fillPoly(mask, [points], 1)
        return mask.astype(bool)
    except:
        return None


def get_mask_from_threshold(img_data):
    """è®¡ç®—å¼ºåº¦å¹¶è¿›è¡Œé˜ˆå€¼è¿‡æ»¤"""
    intensity = np.mean(img_data, axis=2)
    limit = np.max(intensity) * THRESHOLD_RATIO
    return intensity > limit


from scipy.signal import savgol_filter


def preprocess_spectra(pixels, use_snv=True, use_savgol=True, use_derivative=False):
    """
    ç»¼åˆé¢„å¤„ç†ç®¡é“
    pixels: (N, Bands)
    """
    data = pixels.copy()

    # 1. Savitzky-Golay å¹³æ»‘ (å»å™ª)
    if use_savgol:
        # window_length éœ€æ ¹æ®æ³¢æ®µé—´éš”è°ƒæ•´ï¼Œé€šå¸¸ 5-11 ä¹‹é—´
        data = savgol_filter(data, window_length=9, polyorder=2, axis=1)

    # 2. ä¸€é˜¶å¯¼æ•° (å¯é€‰ï¼Œçªå‡ºç‰¹å¾)
    if use_derivative:
        data = np.gradient(data, axis=1)

    # 3. å½’ä¸€åŒ– (SNV ä¼˜äº MinMax)
    if use_snv:
        mean = np.mean(data, axis=1, keepdims=True)
        std = np.std(data, axis=1, keepdims=True)
        std[std == 0] = 1e-6
        data = (data - mean) / std
    else:
        # å¦‚æœä»åšæŒç”¨ MinMaxï¼Œå»ºè®®å…ˆå¹³æ»‘å† MinMax
        p_min = data.min(axis=1, keepdims=True)
        p_max = data.max(axis=1, keepdims=True)
        rng = p_max - p_min
        rng[rng == 0] = 1e-6
        data = (data - p_min) / rng

    return data.astype(np.float32)


def filter_outliers(pixels, labels, purity_threshold=0.90):
    """
    åŸºäºå…‰è°±è§’çš„ç¦»ç¾¤ç‚¹å‰”é™¤ (ç®€å•ç‰ˆ)
    """
    clean_pixels = []
    clean_labels = []

    unique_labels = np.unique(labels)
    for lbl in unique_labels:
        idx = np.where(labels == lbl)[0]
        cls_pixels = pixels[idx]

        # è®¡ç®—è¯¥ç±»å¹³å‡å…‰è°± (Centroid)
        centroid = np.mean(cls_pixels, axis=0)
        norm_centroid = np.linalg.norm(centroid)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)
        # A . B / (|A| * |B|)
        norms = np.linalg.norm(cls_pixels, axis=1)
        dots = np.dot(cls_pixels, centroid)
        sims = dots / (norms * norm_centroid + 1e-6)

        # ä¿ç•™ç›¸ä¼¼åº¦é«˜çš„çº¯å‡€åƒç´ 
        mask = sims >= purity_threshold
        clean_pixels.append(cls_pixels[mask])
        clean_labels.append(labels[idx][mask])

        print(f"   ğŸ§¹ Class {lbl}: å‰”é™¤ {len(idx) - np.sum(mask)} ä¸ªç¦»ç¾¤æ‚è´¨åƒç´ ")

    return np.vstack(clean_pixels), np.concatenate(clean_labels)



def determine_label(path_string):
    """
    [ä¿®æ”¹] æ ¸å¿ƒæ ‡ç­¾åˆ¤æ–­é€»è¾‘
    PET -> 1
    PA (å°¼é¾™) -> 2 (ç‹¬ç«‹ç±»åˆ«)
    å…¶ä»–éPET -> 0
    """
    path_upper = path_string.upper()

    # 1. ä¼˜å…ˆåˆ¤æ–­ PET
    if "PET" in path_upper and "NO_PET" not in path_upper and "NO-PET" not in path_upper:
        return LABEL_MAP["PET"], "PET"

    # 2. [æ–°å¢] ä¸“é—¨åˆ¤æ–­ PA (å°¼é¾™)
    # åªè¦è·¯å¾„æˆ–æ–‡ä»¶åä¸­åŒ…å« PAï¼Œå°±å½’ä¸ºç±»åˆ« 2
    if "PA" in path_upper:
        return LABEL_MAP["PA"], "PA"

    # 3. å…¶ä»–è´Ÿæ ·æœ¬åˆ¤æ–­
    # æ³¨æ„ï¼šPA å·²ç»ä»è¿™ä¸ªåˆ—è¡¨ä¸­ç§»é™¤ï¼Œæˆ–è€…ä¸Šé¢çš„ if "PA" ä¼šå…ˆæ‹¦æˆª
    negative_keys = ["CC", "PP", "é†‹é…¸", "OTHER", "NO_PET", "NO-PET"]
    for key in negative_keys:
        if key in path_upper:
            return LABEL_MAP["NON_PET"], "NON_PET"

    return None, None


def process_and_save_data():
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)

    print("ğŸ“¥ æ­£åœ¨åŠ è½½é»‘ç™½æ ¡å‡†æ–‡ä»¶...")
    white_ref, dark_ref = load_calibration_data(WHITE_REF_PATH, DARK_REF_PATH)

    all_pixels, all_labels = [], []
    # [ä¿®æ”¹] å¢åŠ ç±»åˆ« 2 çš„ç»Ÿè®¡æ§½ä½
    stats = {0: 0, 1: 0, 2: 0}
    total_files = 0

    print(f"ğŸš€ [ä¸“å®¶æ¨¡å¼] å¼€å§‹å¤„ç†æ•°æ® (å·²å¯ç”¨ PA ç‹¬ç«‹åˆ†ç±»)")
    print(f"ğŸ“‚ æ­£åœ¨æ‰«æ {len(DATASETS)} ä¸ªæ•°æ®æº...")

    for ds_config in DATASETS:
        spe_dir, json_dir = ds_config["spe_dir"], ds_config.get("json_dir")
        if not os.path.exists(spe_dir): continue

        hdr_files = glob.glob(os.path.join(spe_dir, "**", "*.hdr"), recursive=True)

        for idx, hdr_path in enumerate(hdr_files):
            if "ref" in os.path.basename(hdr_path).lower(): continue

            full_path_str = hdr_path
            label_id, label_name = determine_label(full_path_str)

            if label_id is None: continue

            img_data = load_envi_image_with_calibration(hdr_path, white_ref, dark_ref)
            if img_data is None: continue

            # è·å–æ©è†œ
            fg_mask = None
            mode = "Threshold"

            if json_dir:
                base = os.path.splitext(os.path.basename(hdr_path))[0]
                jp = os.path.join(json_dir, base + ".json")
                if os.path.exists(jp):
                    json_mask = get_mask_from_json(jp, img_data.shape)
                    if json_mask is not None:
                        thresh_mask = get_mask_from_threshold(img_data)
                        fg_mask = json_mask & thresh_mask
                        mode = "JSON+Threshold"

            if fg_mask is None:
                fg_mask = get_mask_from_threshold(img_data)
                mode = "Auto-Threshold"

            valid_pixels = img_data[fg_mask]

            if len(valid_pixels) > 0:
                if len(valid_pixels) > SAMPLES_PER_IMAGE:
                    indices = np.random.choice(len(valid_pixels), SAMPLES_PER_IMAGE, replace=False)
                    valid_pixels = valid_pixels[indices]

                norm_pixels = min_max_normalize(valid_pixels)

                all_pixels.append(norm_pixels)
                all_labels.append(np.full(len(norm_pixels), label_id, dtype=np.int32))
                stats[label_id] += len(norm_pixels)

            total_files += 1
            if idx % 20 == 0:
                print(
                    f"   [{idx + 1}] {os.path.basename(hdr_path):<20} | ğŸ·ï¸ {label_name}({label_id}) | âš™ï¸ {mode} | æ ·æœ¬æ•°: {len(valid_pixels)}")

    if not all_pixels:
        print("âŒ æ— æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    print("\nğŸ“¦ åˆå¹¶æ•°æ®...")
    X = np.vstack(all_pixels)
    y = np.concatenate(all_labels)

    perm = np.random.permutation(len(y))
    X, y = X[perm], y[perm]

    print("-" * 30)
    print(f"âœ… å®Œæˆ! æ€»æ–‡ä»¶: {total_files}")
    print(f"ğŸ“Š æ­£æ ·æœ¬   (PET, Label 1): {stats[1]}")
    print(f"ğŸ“Š æ™®é€šè´Ÿæ · (CC/Label 0):   {stats[0]}")
    print(f"ğŸ“Š å›°éš¾è´Ÿæ · (PA/Label 2):   {stats[2]} <--- ç¡®è®¤è¿™é‡Œæœ‰æ•°æ®!")

    np.save(os.path.join(OUTPUT_DIR, "X.npy"), X)
    np.save(os.path.join(OUTPUT_DIR, "y.npy"), y)
    print(f"ğŸ’¾ å·²ä¿å­˜è‡³ {OUTPUT_DIR}")


if __name__ == "__main__":
    process_and_save_data()