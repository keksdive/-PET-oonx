import os
import numpy as np
import spectral.io.envi as envi
import glob
import json
import cv2
import random
import time
from scipy.signal import savgol_filter
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# ================= âš™ï¸ 1. å¤šæºæ•°æ®é›†é…ç½® (ä¸“å®¶æ¨¡å¼) =================

DATASETS = [
    # --- æ­£æ ·æœ¬ (PET) ---
    {
        "spe_dir": r"D:\Train_Data\fake_img\train-PET",
        "json_dir": r"D:\Train_Data\fake_img\train-PET\fake_images",
        "label_id": 1,  # PET
        "name": "PET"
    },
    # --- è´Ÿæ ·æœ¬ (CC - ç¢³é…¸é’™) ---
    {
        "spe_dir": r"D:\Train_Data\no_PET\CC",
        "json_dir": None,
        "label_id": 3,  # CC
        "name": "CC"
    },
    # --- è´Ÿæ ·æœ¬ (PA - å°¼é¾™) ---
    {
        "spe_dir": r"D:\Train_Data\no_PET\PA",
        "json_dir": None,
        "label_id": 2,  # PA
        "name": "PA"
    }
]

# æ ¡å‡†æ–‡ä»¶è·¯å¾„
WHITE_REF_PATH = r"D:\Train_Data\DWA\white_ref.spe"
DARK_REF_PATH = r"D:\Train_Data\DWA\dark_ref.spe"

# è¾“å‡ºä¿å­˜è·¯å¾„
OUTPUT_DIR = r"D:\Processed_Result\material-feature"

# é‡‡æ ·ä¸æ¸…æ´—å‚æ•°
SAMPLES_PER_IMAGE = 5000
TARGET_BANDS = 208
PURITY_THRESHOLD = 0.80
SAVGOL_WINDOW = 11
SAVGOL_POLY = 3


# ================= ğŸ› ï¸ 2. æ ¸å¿ƒç®—æ³•å·¥å…·åº“ =================

def apply_snv(spectra):
    spectra = spectra.astype(np.float32)
    mean = np.mean(spectra, axis=1, keepdims=True)
    std = np.std(spectra, axis=1, keepdims=True)
    std[std == 0] = 1e-6
    return (spectra - mean) / std


def apply_derivative(spectra, window=11, poly=3):
    return savgol_filter(spectra, window_length=window, polyorder=poly, deriv=1, axis=1)


def filter_impurities(pixels, label_name, threshold=0.95):
    if len(pixels) == 0:
        return pixels
    mean_spectrum = np.mean(pixels, axis=0).reshape(1, -1)
    similarities = cosine_similarity(pixels, mean_spectrum)
    mask = similarities.flatten() >= threshold
    clean_pixels = pixels[mask]
    drop_rate = (1 - len(clean_pixels) / len(pixels)) * 100
    print(f"   ğŸ§¹ [{label_name}] æ¸…æ´—: åŸå§‹ {len(pixels)} -> ä¿ç•™ {len(clean_pixels)} (å‰”é™¤ç‡ {drop_rate:.1f}%)")
    return clean_pixels


def repair_hdr_file(hdr_path):
    try:
        with open(hdr_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        content = "".join(lines).lower()
        if "byte order" not in content:
            lines.append("\nbyte order = 0\n")
            with open(hdr_path, 'w', encoding='utf-8') as f:
                f.writelines(lines)
    except:
        pass


def load_calibration_data(white_path, dark_path):
    def load_mean(path):
        hdr = os.path.splitext(path)[0] + ".hdr"
        repair_hdr_file(hdr)
        img = envi.open(hdr, path).load()
        return np.mean(img, axis=(0, 1)).astype(np.float32)

    w = load_mean(white_path)
    d = load_mean(dark_path)
    return w, d


def load_envi_image_reflectance(hdr_path, white_ref, dark_ref):
    try:
        repair_hdr_file(hdr_path)
        base = os.path.splitext(hdr_path)[0]
        spe_path = base + ".spe"
        if not os.path.exists(spe_path): spe_path = base + ".raw"
        img_obj = envi.open(hdr_path, spe_path)
        raw_data = np.array(img_obj.load(), dtype=np.float32)
        if raw_data.shape[1] < raw_data.shape[2] and raw_data.shape[1] in [206, 208, 224]:
            raw_data = np.transpose(raw_data, (0, 2, 1))
        H, W, B = raw_data.shape
        w_res = cv2.resize(white_ref.reshape(1, -1), (B, 1)).flatten() if white_ref.shape[0] != B else white_ref
        d_res = cv2.resize(dark_ref.reshape(1, -1), (B, 1)).flatten() if dark_ref.shape[0] != B else dark_ref
        denom = w_res - d_res
        denom[denom == 0] = 1e-6
        reflectance = (raw_data - d_res) / denom
        if TARGET_BANDS is not None and B != TARGET_BANDS:
            flat = reflectance.reshape(-1, B)
            flat_resized = cv2.resize(flat, (TARGET_BANDS, H * W), interpolation=cv2.INTER_LINEAR)
            reflectance = flat_resized.reshape(H, W, TARGET_BANDS)
        return reflectance
    except Exception as e:
        print(f"âŒ Error {os.path.basename(hdr_path)}: {e}")
        return None


def get_mask_combined(json_path, img_data):
    H, W = img_data.shape[:2]
    intensity = np.mean(img_data, axis=2)
    thresh_mask = intensity > 0.10
    json_mask = None
    if json_path and os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            j_mask = np.zeros((H, W), dtype=np.uint8)
            for shape in data.get('shapes', []):
                points = np.array(shape['points'], dtype=np.int32)
                cv2.fillPoly(j_mask, [points], 1)
            json_mask = j_mask.astype(bool)
        except:
            pass
    return (json_mask & thresh_mask) if json_mask is not None else thresh_mask


def generate_cleaning_report(X, y, label_names, output_dir):
    """éªŒè¯å¤„ç†åå„ç±»æè´¨çš„ç‰©ç†å¯åˆ†æ€§"""
    print("\nğŸ“Š [è¯„ä¼°] æ­£åœ¨ç”Ÿæˆæè´¨å¯åˆ†æ€§æ··æ·†çŸ©é˜µ...")
    unique_labels = np.unique(y)
    centroids = [np.mean(X[y == label], axis=0) for label in unique_labels]
    centroids = np.array(centroids)
    sim_matrix = cosine_similarity(X, centroids)
    y_pred = unique_labels[np.argmax(sim_matrix, axis=1)]

    cm = confusion_matrix(y, y_pred)
    cm_perc = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_perc, annot=True, fmt='.2%', cmap='Blues', xticklabels=label_names, yticklabels=label_names)
    plt.title('Material Separability Confusion Matrix (After Cleaning)')
    plt.ylabel('True Material')
    plt.xlabel('Predicted (Nearest Centroid)')
    plot_path = os.path.join(output_dir, "cleaning_confusion_matrix.png")
    plt.savefig(plot_path)
    plt.close()

    report = classification_report(y, y_pred, target_names=label_names)
    with open(os.path.join(output_dir, "cleaning_report.txt"), "w", encoding='utf-8') as f:
        f.write("=== Data Cleaning & Separability Report ===\n")
        f.write(f"Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(report)
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_dir}")



def generate_cleaning_report(X, y, label_names, output_dir):
    """
    [æ–°å¢åŠŸèƒ½] ç”Ÿæˆæ¸…æ´—åæ•°æ®çš„æ··æ·†çŸ©é˜µå’Œåˆ†ç±»æŠ¥å‘Š
    éªŒè¯ SNV + å¯¼æ•°å¤„ç†åï¼Œå„ç±»æè´¨çš„ç‰©ç†å¯åˆ†æ€§
    """
    print("\nğŸ“Š [è¯„ä¼°] æ­£åœ¨ç”Ÿæˆæè´¨å¯åˆ†æ€§æ··æ·†çŸ©é˜µ...")

    # 1. è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è´¨å¿ƒ (Centroids)
    unique_labels = np.unique(y)
    centroids = []
    for label in unique_labels:
        # è®¡ç®—è¯¥ç±»åˆ«æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å…‰è°±
        centroids.append(np.mean(X[y == label], axis=0))
    centroids = np.array(centroids)

    # 2. ç®€æ˜“è´¨å¿ƒåˆ†ç±»å™¨é¢„æµ‹ (åŸºäºä½™å¼¦ç›¸ä¼¼åº¦)
    # è®¡ç®—æ‰€æœ‰æ ·æœ¬ä¸æ‰€æœ‰è´¨å¿ƒçš„ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = cosine_similarity(X, centroids)
    # å–ç›¸ä¼¼åº¦æœ€é«˜çš„è´¨å¿ƒä½œä¸ºé¢„æµ‹ç±»åˆ«
    y_pred = unique_labels[np.argmax(sim_matrix, axis=1)]

    # 3. è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y, y_pred)
    # å½’ä¸€åŒ– (ç™¾åˆ†æ¯”è¡¨ç¤º)
    cm_perc = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    # 4. ç»˜å›¾å¹¶ä¿å­˜ (è¿™é‡Œå®šä¹‰äº† plot_path)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_perc, annot=True, fmt='.2%', cmap='Blues',
                xticklabels=label_names, yticklabels=label_names)
    plt.title('Material Separability Confusion Matrix (After Cleaning)')
    plt.ylabel('True Material')
    plt.xlabel('Predicted (Nearest Centroid)')

    plot_path = os.path.join(output_dir, "cleaning_confusion_matrix.png")
    plt.savefig(plot_path)
    plt.close()

    # 5. ä¿å­˜è¯¦ç»†æ–‡æœ¬æŠ¥å‘Š (æ‚¨æä¾›çš„ç‰‡æ®µéƒ¨åˆ†)
    report = classification_report(y, y_pred, target_names=label_names)
    with open(os.path.join(output_dir, "cleaning_report.txt"), "w", encoding='utf-8') as f:
        f.write("=== Data Cleaning & Separability Report ===\n")
        f.write(f"Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(report)

    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {plot_path}")
    print(f"âœ… æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜è‡³: cleaning_report.txt")

# ================= ğŸš€ 3. ä¸»å¤„ç†æµç¨‹ =================

def process_and_save_data():
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)
    white_ref, dark_ref = load_calibration_data(WHITE_REF_PATH, DARK_REF_PATH)
    raw_data_buffer = {}

    print(f"\nğŸ”„ [é˜¶æ®µ1] æ‰«æå¹¶æå–åŸå§‹åƒç´ ...")
    for ds_config in DATASETS:
        label_id = ds_config["label_id"]
        label_name = ds_config["name"]
        if label_id not in raw_data_buffer: raw_data_buffer[label_id] = []

        print(f"   ğŸ“‚ æ­£åœ¨å¤„ç†: {label_name}...")
        hdr_files = glob.glob(os.path.join(ds_config["spe_dir"], "**", "*.hdr"), recursive=True)
        for hdr_path in hdr_files:
            if "ref" in os.path.basename(hdr_path).lower(): continue
            img_data = load_envi_image_reflectance(hdr_path, white_ref, dark_ref)
            if img_data is None: continue

            base = os.path.splitext(os.path.basename(hdr_path))[0]
            json_path = os.path.join(ds_config.get("json_dir", ""), base + ".json") if ds_config.get(
                "json_dir") else None
            mask = get_mask_combined(json_path, img_data)
            valid_pixels = img_data[mask]

            if len(valid_pixels) > SAMPLES_PER_IMAGE:
                valid_pixels = valid_pixels[np.random.choice(len(valid_pixels), SAMPLES_PER_IMAGE, replace=False)]
            if len(valid_pixels) > 0: raw_data_buffer[label_id].append(valid_pixels)

    print(f"\nğŸ§¹ [é˜¶æ®µ2] æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å·¥ç¨‹...")
    final_X, final_y, names = [], [], []
    for label_id, pixel_list in raw_data_buffer.items():
        if not pixel_list: continue
        all_pixels = np.vstack(pixel_list)
        label_name = [d['name'] for d in DATASETS if d['label_id'] == label_id][0]
        snv_pixels = apply_snv(all_pixels)
        clean_snv_pixels = filter_impurities(snv_pixels, label_name, threshold=PURITY_THRESHOLD)
        if len(clean_snv_pixels) == 0: continue

        deriv_pixels = apply_derivative(clean_snv_pixels, window=SAVGOL_WINDOW, poly=SAVGOL_POLY)
        stacked_features = np.concatenate([clean_snv_pixels, deriv_pixels], axis=1)
        final_X.append(stacked_features)
        final_y.append(np.full(len(stacked_features), label_id, dtype=np.int32))
        names.append(label_name)

    if not final_X: return
    X, y = np.vstack(final_X), np.concatenate(final_y)
    perm = np.random.permutation(len(y))
    X, y = X[perm], y[perm]

    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡: {len(y)} æ ·æœ¬, {X.shape[1]} ç»´åº¦")
    # è°ƒç”¨ç”ŸæˆæŠ¥å‘Šå‡½æ•°
    generate_cleaning_report(X, y, names, OUTPUT_DIR)

    np.save(os.path.join(OUTPUT_DIR, "X.npy"), X)
    np.save(os.path.join(OUTPUT_DIR, "y.npy"), y)
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {OUTPUT_DIR}")


if __name__ == "__main__":
    process_and_save_data()