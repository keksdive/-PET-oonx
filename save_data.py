import os
import numpy as np
import spectral.io.envi as envi
import glob
import json
import cv2
import random

# ================= âš™ï¸ å¤šæºæ•°æ®é›†é…ç½®åŒºåŸŸ =================
DATASETS = [
    # 1. PET æ–‡ä»¶å¤¹ (æ­£æ ·æœ¬)
    {
        "spe_dir": r"E:\SPEDATA\é«˜è°±ç›¸æœºæ•°æ®é›†\è®­ç»ƒé›†\ç½®ä¿¡åº¦å¤§äº90%PET",
        "json_dir": None
    },
    # 2. é PET æ–‡ä»¶å¤¹ (CC) -> å°†ä½œä¸ºè´Ÿæ ·æœ¬
    {
        "spe_dir": r"E:\SPEDATA\é«˜è°±ç›¸æœºæ•°æ®é›†\è®­ç»ƒé›†\no_PET\CC",
        "json_dir": None
    },
    # 3. é PET æ–‡ä»¶å¤¹ (PA) -> å°†ä½œä¸ºè´Ÿæ ·æœ¬
    {
        "spe_dir": r"E:\SPEDATA\é«˜è°±ç›¸æœºæ•°æ®é›†\è®­ç»ƒé›†\no_PET\PA",
        "json_dir": None
    }
]

# [æ–°å¢] æ ¡å‡†æ–‡ä»¶è·¯å¾„ (è¯·ç¡®è®¤è·¯å¾„æ˜¯å¦æ­£ç¡®)
WHITE_REF_PATH = r"E:\SPEDATA\é«˜è°±ç›¸æœºæ•°æ®é›†\DWA\white_ref.spe"
DARK_REF_PATH = r"E:\SPEDATA\é«˜è°±ç›¸æœºæ•°æ®é›†\DWA\dark_ref.spe"

# è¾“å‡ºä¿å­˜è·¯å¾„
OUTPUT_DIR = r"E:\SPEDATA\NP_new1.0.2"

# æ ‡ç­¾å®šä¹‰ï¼šäºŒåˆ†ç±»é€»è¾‘
LABEL_MAP = {
    "PET": 1,
    "NON_PET": 0
}

# é‡‡æ ·å‚æ•°
SAMPLES_PER_IMAGE = 3000
THRESHOLD_RATIO = 0.15
TARGET_BANDS = 208  # å¼ºåˆ¶å¯¹é½æ³¢æ®µæ•°


# =======================================================

def repair_hdr_file(hdr_path):
    """è‡ªåŠ¨ä¿®å¤ç¼ºå°‘çš„ byte order"""
    try:
        with open(hdr_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        content = "".join(lines).lower()
        if "byte order" not in content:
            lines.append("\nbyte order = 0\n")
            with open(hdr_path, 'w', encoding='utf-8') as f:
                f.writelines(lines)
    except:
        pass


def load_calibration_data(white_path, dark_path):
    """
    [æ–°å¢] åŠ è½½é»‘ç™½æ ¡å‡†æ–‡ä»¶å¹¶è®¡ç®—å¹³å‡å…‰è°±
    è¿”å›: (white_mean, dark_mean) ç»´åº¦ä¸º (Bands,)
    """
    print(f"âšª åŠ è½½ç™½æ¿: {white_path}")
    print(f"âš« åŠ è½½é»‘æ¿: {dark_path}")

    def load_mean(path):
        hdr = os.path.splitext(path)[0] + ".hdr"
        repair_hdr_file(hdr)
        if not os.path.exists(path) or not os.path.exists(hdr):
            raise FileNotFoundError(f"ç¼ºå¤±æ ¡å‡†æ–‡ä»¶: {path}")
        img = envi.open(hdr, path).load()
        # è®¡ç®—ç©ºé—´ç»´åº¦çš„å¹³å‡å€¼ï¼Œå¾—åˆ°çº¯å…‰è°±å‘é‡
        return np.mean(img, axis=(0, 1)).astype(np.float32)

    try:
        w = load_mean(white_path)
        d = load_mean(dark_path)
        return w, d
    except Exception as e:
        print(f"âŒ æ ¡å‡†æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        exit()


def load_envi_image_with_calibration(hdr_path, white_ref, dark_ref):
    """
    [ä¿®æ”¹] åŠ è½½ ENVI å›¾åƒå¹¶ç«‹å³æ‰§è¡Œé»‘ç™½æ ¡æ­£
    Reflectance = (Raw - Dark) / (White - Dark)
    """
    try:
        repair_hdr_file(hdr_path)
        base = os.path.splitext(hdr_path)[0]
        spe_path = base + ".spe"
        if not os.path.exists(spe_path): spe_path = base + ".raw"
        if not os.path.exists(spe_path): return None

        # 1. åŠ è½½åŸå§‹ RAW æ•°æ® (DNå€¼)
        img_obj = envi.open(hdr_path, spe_path)
        raw_data = np.array(img_obj.load(), dtype=np.float32)

        # 2. ç»´åº¦ä¿®æ­£ (ç¡®ä¿æ˜¯ H, W, B)
        shape = raw_data.shape
        if shape[1] < shape[2] and shape[1] in [206, 208, 224]:
            raw_data = np.transpose(raw_data, (0, 2, 1))

        H, W, B = raw_data.shape

        # 3. [æ ¸å¿ƒ] æ‰§è¡Œé»‘ç™½æ ¡æ­£ (åå°„ç‡è®¡ç®—)
        # è‡ªåŠ¨é€‚é…æ ¡å‡†æ–‡ä»¶çš„æ³¢æ®µæ•° (é˜²æ­¢å›  208 vs 224 å¯¼è‡´çš„ crash)
        if white_ref.shape[0] != B:
            # å¦‚æœæ³¢æ®µä¸åŒ¹é…ï¼Œç®€å•çº¿æ€§æ’å€¼æ ¡å‡†æ•°æ®åˆ°å›¾åƒçš„æ³¢æ®µæ•°
            # æ³¨æ„ï¼šè¿™æ˜¯ä¸ºäº†é˜²æ­¢æŠ¥é”™çš„å…œåº•ç­–ç•¥ï¼Œç†æƒ³æƒ…å†µä¸‹åº”ä¸€è‡´
            w_res = cv2.resize(white_ref.reshape(1, -1), (B, 1)).flatten()
            d_res = cv2.resize(dark_ref.reshape(1, -1), (B, 1)).flatten()
        else:
            w_res, d_res = white_ref, dark_ref

        denom = w_res - d_res
        denom[denom == 0] = 1e-6  # é˜²æ­¢é™¤é›¶

        # åˆ©ç”¨å¹¿æ’­æœºåˆ¶è®¡ç®—åå°„ç‡
        reflectance = (raw_data - d_res) / denom

        # è£å‰ªå¼‚å¸¸å€¼ (0~1 ä¹‹å¤–çš„é€šå¸¸æ˜¯å™ªå£°)
        # reflectance = np.clip(reflectance, 0, 1.5) # å¯é€‰ï¼Œæš‚ä¸å¼ºåˆ¶ clipï¼Œä¿ç•™é«˜å…‰ç‰¹å¾

        # 4. æ³¢æ®µå¯¹é½ (Resize åˆ° TARGET_BANDS)
        if TARGET_BANDS is not None and B != TARGET_BANDS:
            flat = reflectance.reshape(-1, B)
            flat_resized = cv2.resize(flat, (TARGET_BANDS, H * W), interpolation=cv2.INTER_LINEAR)
            reflectance = flat_resized.reshape(H, W, TARGET_BANDS)

        return reflectance

    except Exception as e:
        print(f"âŒ åŠ è½½æˆ–æ ¡æ­£å¤±è´¥ {os.path.basename(hdr_path)}: {e}")
        return None


def get_mask_from_json(json_path, image_shape):
    """ä¼˜å…ˆä½¿ç”¨ JSON æ ‡æ³¨"""
    H, W = image_shape[:2]
    mask = np.zeros((H, W), dtype=np.uint8)
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        for shape in data.get('shapes', []):
            points = np.array(shape['points'], dtype=np.int32)
            cv2.fillPoly(mask, [points], 1)
        return mask.astype(bool)
    except:
        return None


def get_mask_from_threshold(img_data):
    """è®¡ç®—å¼ºåº¦å¹¶è¿›è¡Œä¸¥æ ¼é˜ˆå€¼è¿‡æ»¤ (åŸºäºåå°„ç‡)"""
    intensity = np.mean(img_data, axis=2)
    # æ³¨æ„ï¼šåå°„ç‡é€šå¸¸åœ¨ 0~1 ä¹‹é—´ï¼Œæ‰€ä»¥é˜ˆå€¼é€»è¾‘ä¾ç„¶é€‚ç”¨
    # ä½†å¦‚æœåå…‰å¾ˆå¼ºå¯èƒ½ >1ï¼Œå– max * ratio ä¾ç„¶æ˜¯ç¨³å¥çš„
    limit = np.max(intensity) * THRESHOLD_RATIO
    return intensity > limit


def min_max_normalize(pixels):
    """
    Min-Max å½’ä¸€åŒ– (é’ˆå¯¹åƒç´ çº§)
    è™½ç„¶å·²ç»æ˜¯åå°„ç‡äº†ï¼Œä½†ä¸ºäº†è¾“å…¥ç¥ç»ç½‘ç»œï¼Œå†æ¬¡å½’ä¸€åŒ–åˆ° 0-1 ä¹Ÿæ˜¯å¸¸è§çš„åšæ³•
    """
    p_min = pixels.min(axis=1, keepdims=True)
    p_max = pixels.max(axis=1, keepdims=True)
    range_val = p_max - p_min
    range_val[range_val == 0] = 1e-6
    return (pixels - p_min) / range_val


def determine_label(path_string):
    path_upper = path_string.upper()

    if "PET" in path_upper and "NO_PET" not in path_upper and "NO-PET" not in path_upper:
        return LABEL_MAP["PET"], "PET"

    negative_keys = ["CC", "PA", "PP", "é†‹é…¸", "OTHER", "NO_PET", "NO-PET"]
    for key in negative_keys:
        if key in path_upper:
            return LABEL_MAP["NON_PET"], "NON_PET"

    return None, None


def process_and_save_data():
    if not os.path.exists(OUTPUT_DIR): os.makedirs(OUTPUT_DIR)

    # 1. [æ–°å¢] é¢„åŠ è½½æ ¡å‡†æ•°æ®
    print("ğŸ“¥ æ­£åœ¨åŠ è½½é»‘ç™½æ ¡å‡†æ–‡ä»¶...")
    white_ref, dark_ref = load_calibration_data(WHITE_REF_PATH, DARK_REF_PATH)

    all_pixels, all_labels = [], []
    stats = {0: 0, 1: 0}
    total_files = 0

    print(f"ğŸš€ [ä¸“å®¶æ¨¡å¼] å¼€å§‹å¤„ç†æ•°æ® (å·²å¯ç”¨é»‘ç™½è¾å°„æ ¡æ­£)")
    print(f"ğŸ“‚ æ­£åœ¨æ‰«æ {len(DATASETS)} ä¸ªæ•°æ®æº...")

    for ds_config in DATASETS:
        spe_dir, json_dir = ds_config["spe_dir"], ds_config.get("json_dir")
        if not os.path.exists(spe_dir): continue

        hdr_files = glob.glob(os.path.join(spe_dir, "**", "*.hdr"), recursive=True)

        for idx, hdr_path in enumerate(hdr_files):
            if "ref" in os.path.basename(hdr_path).lower(): continue

            full_path_str = hdr_path
            label_id, label_name = determine_label(full_path_str)

            if label_id is None: continue

            # [ä¿®æ”¹] è°ƒç”¨å¸¦æ ¡æ­£çš„åŠ è½½å‡½æ•°
            img_data = load_envi_image_with_calibration(hdr_path, white_ref, dark_ref)
            if img_data is None: continue

            # 1. è·å–æœ‰æ•ˆåŒºåŸŸæ©è†œ
            fg_mask = None
            mode = "Threshold"

            if json_dir:
                base = os.path.splitext(os.path.basename(hdr_path))[0]
                jp = os.path.join(json_dir, base + ".json")
                if os.path.exists(jp):
                    json_mask = get_mask_from_json(jp, img_data.shape)
                    if json_mask is not None:
                        thresh_mask = get_mask_from_threshold(img_data)
                        fg_mask = json_mask & thresh_mask
                        mode = "JSON+Threshold"

            if fg_mask is None:
                fg_mask = get_mask_from_threshold(img_data)
                mode = "Auto-Threshold"

            # 2. æå–åƒç´ 
            valid_pixels = img_data[fg_mask]

            if len(valid_pixels) > 0:
                if len(valid_pixels) > SAMPLES_PER_IMAGE:
                    indices = np.random.choice(len(valid_pixels), SAMPLES_PER_IMAGE, replace=False)
                    valid_pixels = valid_pixels[indices]

                # 3. å½’ä¸€åŒ– (åå°„ç‡å·²ç»æ˜¯ç‰©ç†é‡ï¼Œä½†ä¸ºäº†ç¥ç»ç½‘ç»œç¨³å®šæ€§ï¼Œå†æ¬¡å½’ä¸€åŒ–)
                norm_pixels = min_max_normalize(valid_pixels)

                # 4. ä¿å­˜
                all_pixels.append(norm_pixels)
                all_labels.append(np.full(len(norm_pixels), label_id, dtype=np.int32))
                stats[label_id] += len(norm_pixels)

            total_files += 1
            if idx % 20 == 0:
                print(
                    f"   [{idx + 1}] {os.path.basename(hdr_path):<20} | ğŸ·ï¸ {label_name}({label_id}) | âš™ï¸ {mode} | æ ·æœ¬æ•°: {len(valid_pixels)}")

    if not all_pixels:
        print("âŒ æ— æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    print("\nğŸ“¦ åˆå¹¶æ•°æ®...")
    X = np.vstack(all_pixels)
    y = np.concatenate(all_labels)

    # æ‰“ä¹±æ•°æ®
    perm = np.random.permutation(len(y))
    X, y = X[perm], y[perm]

    print("-" * 30)
    print(f"âœ… å®Œæˆ! æ€»æ–‡ä»¶: {total_files}")
    print(f"ğŸ“Š æ­£æ ·æœ¬ (PET, Label 1): {stats[1]}")
    print(f"ğŸ“Š è´Ÿæ ·æœ¬ (CC/PA/æ‚æ³¢, Label 0): {stats[0]}")

    # æ£€æŸ¥æ•°å€¼èŒƒå›´
    print(f"ğŸ“‰ æ•°æ®èŒƒå›´: Min={X.min():.4f}, Max={X.max():.4f}")
    if X.max() > 1.0 or X.min() < 0.0:
        print("âš ï¸ è­¦å‘Š: æ•°æ®èŒƒå›´è¶…å‡º 0-1ï¼Œå¯èƒ½ Min-Max å½’ä¸€åŒ–æœ‰è¯¯æˆ–åŸå§‹åå°„ç‡å¼‚å¸¸é«˜")

    np.save(os.path.join(OUTPUT_DIR, "X.npy"), X)
    np.save(os.path.join(OUTPUT_DIR, "y.npy"), y)
    print(f"ğŸ’¾ å·²ä¿å­˜è‡³ {OUTPUT_DIR}")


if __name__ == "__main__":
    process_and_save_data()