import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, losses
import random
from collections import deque


class QNetwork(tf.keras.Model):
    """
    åŸºç¡€ Q ç½‘ç»œç»“æ„
    è¾“å…¥: å…¨å±€çŠ¶æ€ (State Mask)
    è¾“å‡º: å±€éƒ¨åŠ¨ä½œç©ºé—´çš„ Q å€¼ (Local Q-values)
    """

    def __init__(self, num_actions):
        super(QNetwork, self).__init__()
        self.d1 = layers.Dense(256, activation='relu')
        self.d2 = layers.Dense(256, activation='relu')
        self.d3 = layers.Dense(128, activation='relu')
        # åŠ å…¥ Attention æœºåˆ¶ (æ”¹è¿›ç‚¹äºŒçš„è½»é‡åŒ–å®ç°)
        self.attn = layers.Dense(128, activation='sigmoid')
        self.out = layers.Dense(num_actions, activation=None)

    def call(self, inputs):
        x = self.d1(inputs)
        x = self.d2(x)

        # ç®€å•çš„ Self-Attention æ¨¡æ‹Ÿ
        a = self.attn(x)
        x = x * a

        x = self.d3(x)
        return self.out(x)


class SubspaceAgent:
    """
    å­ç©ºé—´æ™ºèƒ½ä½“ï¼šåªè´Ÿè´£ç‰¹å®šæ³¢æ®µèŒƒå›´çš„å†³ç­–
    """

    def __init__(self, global_num_bands, action_range, name="Agent"):
        self.name = name
        self.global_num_bands = global_num_bands
        self.action_start, self.action_end = action_range
        self.num_local_actions = self.action_end - self.action_start

        self.gamma = 0.99
        self.batch_size = 64
        self.memory = deque(maxlen=20000)  # æ¯ä¸ªæ™ºèƒ½ä½“æœ‰è‡ªå·±çš„ç»éªŒæ± 

        # ç½‘ç»œåˆå§‹åŒ–
        self.model = QNetwork(self.num_local_actions)
        self.target_model = QNetwork(self.num_local_actions)

        # Build
        dummy = tf.zeros((1, global_num_bands))
        self.model(dummy)
        self.target_model(dummy)

        self.optimizer = optimizers.Nadam(learning_rate=1e-4, clipnorm=1.0)
        self.loss_fn = losses.Huber()
        self.update_target_network()

    def update_target_network(self):
        self.target_model.set_weights(self.model.get_weights())

    def get_local_q_values(self, state):
        """è·å–å½“å‰çŠ¶æ€ä¸‹ï¼Œè¯¥æ™ºèƒ½ä½“è´Ÿè´£åŒºåŸŸçš„æ‰€æœ‰ Q å€¼"""
        state_tensor = tf.convert_to_tensor(state.reshape(1, -1), dtype=tf.float32)
        return self.model(state_tensor).numpy()[0]

    def train(self):
        if len(self.memory) < self.batch_size:
            return 0

        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = np.array(states, dtype=np.float32)
        next_states = np.array(next_states, dtype=np.float32)
        rewards = np.array(rewards, dtype=np.float32)
        dones = np.array(dones, dtype=np.float32)
        actions = np.array(actions, dtype=np.int32)  # è¿™é‡Œæ˜¯ Local Action Index

        # Double DQN Logic
        next_q_online = self.model.predict(next_states, verbose=0)
        best_local_actions = np.argmax(next_q_online, axis=1)

        next_q_target = self.target_model.predict(next_states, verbose=0)
        rows = np.arange(self.batch_size)
        target_q_values = next_q_target[rows, best_local_actions]

        targets = rewards + self.gamma * target_q_values * (1 - dones)

        with tf.GradientTape() as tape:
            current_q = self.model(states, training=True)
            one_hot = tf.one_hot(actions, self.num_local_actions)
            pred = tf.reduce_sum(current_q * one_hot, axis=1)
            loss = self.loss_fn(targets, pred)

        grads = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
        return loss.numpy()


class MultiAgentManager:
    """
    å¤šæ™ºèƒ½ä½“ç®¡ç†å™¨ï¼šè´Ÿè´£åˆ†å‘ä»»åŠ¡ã€èåˆå†³ç­–
    """

    def __init__(self, total_bands, ranges):
        """
        ranges: list of tuples, e.g., [(0, 70), (70, 140), (140, 208)]
        """
        self.agents = []
        self.ranges = ranges
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995

        for i, r in enumerate(ranges):
            agent = SubspaceAgent(total_bands, r, name=f"Agent_{i}_Range_{r[0]}-{r[1]}")
            self.agents.append(agent)
            print(f"ğŸ¤– åˆå§‹åŒ–æ™ºèƒ½ä½“: {agent.name} (Actions: {agent.num_local_actions})")

    def get_global_action(self, state, selected_bands):
        # 1. æ¢ç´¢
        if np.random.rand() <= self.epsilon:
            # éšæœºä»æ‰€æœ‰æœªé€‰æ³¢æ®µä¸­é€‰ä¸€ä¸ª
            available = [b for b in range(state.shape[0]) if b not in selected_bands]
            return random.choice(available)

        # 2. åˆ©ç”¨ (ååŒå†³ç­–)
        best_global_action = -1
        max_q_value = -float('inf')

        # è¯¢é—®æ¯ä¸ªæ™ºèƒ½ä½“
        for agent in self.agents:
            local_qs = agent.get_local_q_values(state)

            # Masking (åœ¨å±€éƒ¨ Q å€¼ä¸­å±è”½å·²é€‰æ³¢æ®µ)
            for global_idx in selected_bands:
                if agent.action_start <= global_idx < agent.action_end:
                    local_idx = global_idx - agent.action_start
                    local_qs[local_idx] = -float('inf')

            # æ‰¾å‡ºè¯¥æ™ºèƒ½ä½“çš„æœ€ä½³å»ºè®®
            local_best_idx = np.argmax(local_qs)
            q_val = local_qs[local_best_idx]

            # ç«äº‰ï¼šè°çš„ Q å€¼å¤§å¬è°çš„
            if q_val > max_q_value:
                max_q_value = q_val
                best_global_action = agent.action_start + local_best_idx

        return best_global_action

    def remember(self, state, global_action, reward, next_state, done):
        # å°†ç»éªŒåˆ†å‘ç»™è´Ÿè´£è¯¥åŠ¨ä½œçš„æ™ºèƒ½ä½“
        # æ³¨æ„ï¼šåªæœ‰æ‰§è¡Œäº†åŠ¨ä½œçš„æ™ºèƒ½ä½“æ‰éœ€è¦å­¦ä¹ ï¼Œå…¶ä»–æ™ºèƒ½ä½“è¿™è½®"è½®ç©º"
        for agent in self.agents:
            if agent.action_start <= global_action < agent.action_end:
                local_action = global_action - agent.action_start
                agent.memory.append((state, local_action, reward, next_state, done))
                break

    def train(self):
        losses = []
        for agent in self.agents:
            l = agent.train()
            losses.append(l)
        return losses

    def update_targets(self):
        for agent in self.agents:
            agent.update_target_network()

    def decay_epsilon(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay