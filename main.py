import numpy as np
import os
import random
import spectral.io.envi as envi
import cv2
import json
import tensorflow as tf

# å¼•ç”¨ä½ çš„æ¨¡å—
from entropy_utils import precompute_entropies, precompute_mutual_information
from agent import BandSelectionAgent
from reward_utils import calculate_reward

# å¼•å…¥æ•°æ®é¢„å¤„ç†ä¸­çš„ SNV (å¦‚æœä½ çš„ data_preprocessing.py é‡Œæ²¡æœ‰ apply_snvï¼Œè¯·å…ˆæ·»åŠ ï¼Œæˆ–è€…ä½¿ç”¨ä¸‹é¢çš„å†…ç½®å‡½æ•°)

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
TRAIN_DATA_ROOT = r"L:\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰"
WHITE_REF_HDR = r"L:\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\DWA\white_ref.hdr"
DARK_REF_HDR = r"L:\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\DWA\black_ref.hdr"

NUM_BANDS_TO_SELECT = 30
TOTAL_EPISODES = 300
ALPHA = 0.7
SAMPLE_SIZE = 5000

# æ˜¾å­˜é…ç½®
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("âœ… GPU æ˜¾å­˜æŒ‰éœ€åˆ†é…å·²å¼€å¯")
    except RuntimeError as e:
        print(e)


# ===============================================

def apply_snv(spectra):
    """ã€è®ºæ–‡ä¼˜åŒ–ã€‘æ ‡å‡†æ­£æ€å˜é‡å˜æ¢ (SNV)"""
    mean = np.mean(spectra, axis=1, keepdims=True)
    std = np.std(spectra, axis=1, keepdims=True)
    std[std == 0] = 1e-6
    return (spectra - mean) / std


def load_calibration_file(hdr_path):
    base = os.path.splitext(hdr_path)[0]
    spe = base + ".spe"
    if not os.path.exists(spe) and os.path.exists(base): spe = base
    img = envi.open(hdr_path, spe).load()
    if img.shape[1] == 208 and img.shape[2] != 208:
        img = np.transpose(img, (0, 2, 1))
    return np.array(img, dtype=np.float32)


def get_mask_from_json(json_path, img_shape):
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    mask = np.zeros(img_shape, dtype=np.uint8)
    labels_found = []
    for shape in data['shapes']:
        lbl = shape['label'].lower()
        labels_found.append(lbl)
        pts = np.array(shape['points'], dtype=np.int32)
        if 'no_pet' in lbl:
            cv2.fillPoly(mask, [pts], 2)  # NO_PET
        elif 'pet' in lbl:
            cv2.fillPoly(mask, [pts], 1)  # PET

    # è°ƒè¯•ä¿¡æ¯ï¼šå¦‚æœæ²¡æ‰¾åˆ° Maskï¼Œæ‰“å°ä¸€ä¸‹ JSON é‡Œçš„æ ‡ç­¾
    if np.sum(mask) == 0:
        # print(f"  âš ï¸ Warning: {os.path.basename(json_path)} ä¸­æœªåŒ¹é…åˆ° 'pet'/'no_pet'ã€‚åŒ…å«æ ‡ç­¾: {list(set(labels_found))}")
        pass
    return mask


def load_representative_data_for_drl():
    print("ğŸš€ æ­£åœ¨åŠ è½½ DRL è®­ç»ƒæ•°æ®...")
    white = load_calibration_file(WHITE_REF_HDR)
    dark = load_calibration_file(DARK_REF_HDR)
    denom = (white - dark)
    denom[denom == 0] = 1e-6

    collected_X = []
    collected_y = []

    pet_count = 0
    non_pet_count = 0

    for root, dirs, files in os.walk(TRAIN_DATA_ROOT):
        for fname in files:
            if not fname.endswith('.spe'): continue

            json_path = os.path.join(root, fname.replace('.spe', '.json'))
            if not os.path.exists(json_path): continue

            try:
                # å¿«é€Ÿæ£€æŸ¥ JSONï¼Œå¦‚æœè¯¥å›¾æ²¡æœ‰æˆ‘ä»¬è¦çš„æ ‡ç­¾ï¼Œç›´æ¥è·³è¿‡åŠ è½½å›¾åƒï¼ˆçœæ—¶é—´ï¼‰
                with open(json_path, 'r', encoding='utf-8') as f:
                    jdata = json.load(f)
                    has_pet = any(
                        'pet' in s['label'].lower() and 'no_pet' not in s['label'].lower() for s in jdata['shapes'])
                    # å¦‚æœå½“å‰ PET æ ·æœ¬ä¸¥é‡ä¸è¶³ï¼Œä¼˜å…ˆåŠ è½½å« PET çš„å›¾
                    if pet_count < SAMPLE_SIZE // 2 and not has_pet:
                        continue

                # åŠ è½½å›¾åƒ
                hdr_path = os.path.join(root, fname + ".hdr")
                if not os.path.exists(hdr_path): hdr_path = os.path.splitext(os.path.join(root, fname))[0] + ".hdr"

                # ä¿®å¤ Header
                if os.path.exists(hdr_path):
                    with open(hdr_path, 'r', encoding='utf-8', errors='ignore') as f:
                        if 'byte order' not in f.read().lower():
                            with open(hdr_path, 'a') as fa: fa.write('\nbyte order = 0')

                raw = envi.open(hdr_path, os.path.join(root, fname)).load()
                if raw.shape[1] == 208: raw = np.transpose(raw, (0, 2, 1))
                calib = (raw.astype(np.float32) - dark) / denom

                mask = get_mask_from_json(json_path, (calib.shape[0], calib.shape[1]))

                # é‡‡æ · PET (Label 1)
                idx1 = np.where(mask == 1)
                n_p = len(idx1[0])
                if n_p > 0:
                    # åŠ¨æ€è°ƒæ•´é‡‡æ ·é‡ï¼šå¦‚æœ PET ç¼ºå£å¤§ï¼Œå°±å¤šé‡‡ç‚¹
                    needed = (SAMPLE_SIZE // 2) - pet_count
                    take = min(n_p, max(100, needed // 5))  # æ¯æ¬¡æœ€å°‘é‡‡100ï¼Œé™¤éä¸å¤Ÿ
                    indices = np.random.choice(n_p, size=take, replace=False)
                    collected_X.append(calib[idx1[0][indices], idx1[1][indices], :])
                    collected_y.append(np.ones(take))
                    pet_count += take

                # é‡‡æ · NO_PET (Label 2)
                idx2 = np.where(mask == 2)
                n_np = len(idx2[0])
                if n_np > 0 and non_pet_count < SAMPLE_SIZE // 2:
                    take = min(n_np, 100)
                    indices = np.random.choice(n_np, size=take, replace=False)
                    collected_X.append(calib[idx2[0][indices], idx2[1][indices], :])
                    collected_y.append(np.zeros(take))
                    non_pet_count += take

                print(f"  -> è¿›åº¦: PET {pet_count} | Non-PET {non_pet_count} | å½“å‰æ–‡ä»¶: {fname}", end='\r')

                if pet_count >= SAMPLE_SIZE // 2 and non_pet_count >= SAMPLE_SIZE // 2:
                    break

            except Exception as e:
                print(f"\nSkip {fname}: {e}")

        if pet_count >= SAMPLE_SIZE // 2 and non_pet_count >= SAMPLE_SIZE // 2:
            break

    print("\n")
    if not collected_X:
        raise ValueError("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼è¯·æ£€æŸ¥è·¯å¾„å’Œ JSON æ ‡ç­¾ã€‚")

    X = np.concatenate(collected_X, axis=0)
    y = np.concatenate(collected_y, axis=0)

    print(f"âœ… DRL æ•°æ®åŠ è½½ç»Ÿè®¡: æ€»æ•° {len(y)}, PET(1): {np.sum(y == 1)}, èƒŒæ™¯(0): {np.sum(y == 0)}")

    if np.sum(y == 1) == 0:
        raise ValueError("â›”ã€è‡´å‘½é”™è¯¯ã€‘æœªæ£€æµ‹åˆ°ä»»ä½• PET æ ·æœ¬ï¼æ ‡ç­¾åˆ†å¸ƒå…¨æ˜¯ 0ã€‚\n"
                         "è¯·æ£€æŸ¥ï¼š1. JSONæ–‡ä»¶ä¸­ PET çš„æ ‡ç­¾æ˜¯å¦åŒ…å« 'pet' ä¸”ä¸å« 'no_pet'ï¼Ÿ\n"
                         "2. æ˜¯å¦æ‰€æœ‰å›¾ç‰‡çš„ JSON éƒ½åªæœ‰èƒŒæ™¯æ ‡æ³¨ï¼Ÿ")

    return X, y


def train_dqn():
    # 1. å‡†å¤‡æ•°æ®
    X_full, y_full = load_representative_data_for_drl()

    # ã€è®ºæ–‡ä¼˜åŒ–ã€‘åº”ç”¨ SNV é¢„å¤„ç†
    print("ğŸ§ª æ­£åœ¨åº”ç”¨ SNV é¢„å¤„ç† (Paper Optimization)...")
    X_full = apply_snv(X_full)

    num_total_bands = X_full.shape[1]

    # 2. è®¡ç®—æŒ‡æ ‡
    print("ğŸ“Š æ­£åœ¨è®¡ç®— Information Entropy (ç†µ)...")
    entropies = precompute_entropies(X_full)

    print("âš–ï¸ æ­£åœ¨è®¡ç®— Mutual Information (äº’ä¿¡æ¯)...")
    mi_scores = precompute_mutual_information(X_full, y_full)

    # å½’ä¸€åŒ–
    entropies = (entropies - np.min(entropies)) / (np.max(entropies) - np.min(entropies) + 1e-6)
    mi_scores = (mi_scores - np.min(mi_scores)) / (np.max(mi_scores) - np.min(mi_scores) + 1e-6)

    # 3. è®­ç»ƒ Agent
    agent = BandSelectionAgent(num_total_bands)

    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ DRL Agent (Alpha={ALPHA})...")

    # ================= å…³é”®ä¿®å¤ï¼šåˆå§‹åŒ–å˜é‡ =================
    best_reward = -float('inf')
    best_bands = []  # ğŸ‘ˆ ä¹‹å‰æŠ¥é”™å°±æ˜¯å› ä¸ºå°‘äº†è¿™ä¸€è¡Œï¼
    # ======================================================

    for e in range(TOTAL_EPISODES):
        state = np.zeros(num_total_bands)
        selected_bands = []
        episode_reward = 0

        for step in range(NUM_BANDS_TO_SELECT):
            action = agent.get_action(state, selected_bands)
            reward = calculate_reward(selected_bands, action, entropies, mi_scores, alpha=ALPHA)

            next_state = state.copy()
            next_state[action] = 1
            done = (len(selected_bands) == NUM_BANDS_TO_SELECT - 1)

            agent.remember(state, action, reward, next_state, done)
            agent.train()

            state = next_state
            selected_bands.append(action)
            episode_reward += reward

        agent.update_target_network()
        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay

        # æ›´æ–°æœ€ä½³ç»“æœ
        if episode_reward > best_reward:
            best_reward = episode_reward
            best_bands = sorted(selected_bands)

        if (e + 1) % 10 == 0:
            print(
                f"Episode {e + 1}/{TOTAL_EPISODES} | Reward: {episode_reward:.4f} | Epsilon: {agent.epsilon:.2f} | Best: {len(best_bands)} bands")

    print("\n" + "=" * 50)
    print(f"ğŸ† æœ€ç»ˆæ¨èçš„ {len(best_bands)} ä¸ªæ³¢æ®µ (ç´¢å¼•):")
    print(best_bands)
    print("=" * 50)

    return best_bands


if __name__ == "__main__":
    final_bands = train_dqn()

    if not final_bands:
        print("âš ï¸ è­¦å‘Šï¼šè®­ç»ƒæœªè¿”å›æ³¢æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚")
        final_bands = list(range(30))

    # ä¿å­˜ç»“æœç»™ pipeline ä½¿ç”¨
    config_path = "best_bands_config.json"
    save_data = {"selected_bands": [int(b) for b in final_bands]}

    with open(config_path, "w") as f:
        json.dump(save_data, f)

    print(f"ğŸ’¾ [Auto] é…ç½®å·²ä¿å­˜è‡³ {config_path}")