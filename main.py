import numpy as np
import os
import random
import spectral.io.envi as envi
import cv2
import json
import tensorflow as tf

# å¼•ç”¨ä½ çš„æ¨¡å—
from entropy_utils import precompute_entropies, precompute_mutual_information
from agent import BandSelectionAgent
from reward_utils import calculate_reward

# å¼•å…¥æ•°æ®é¢„å¤„ç†ä¸­çš„ SNV (å¦‚æœä½ çš„ data_preprocessing.py é‡Œæ²¡æœ‰ apply_snvï¼Œè¯·å…ˆæ·»åŠ ï¼Œæˆ–è€…ä½¿ç”¨ä¸‹é¢çš„å†…ç½®å‡½æ•°)

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
TRAIN_DATA_ROOT = r"L:\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰"
WHITE_REF_HDR = r"L:\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\DWA\white_ref.hdr"
DARK_REF_HDR = r"L:\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\DWA\black_ref.hdr"

NUM_BANDS_TO_SELECT = 30
TOTAL_EPISODES = 300
ALPHA = 0.7
SAMPLE_SIZE = 5000

# æ˜¾å­˜é…ç½®
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("âœ… GPU æ˜¾å­˜æŒ‰éœ€åˆ†é…å·²å¼€å¯")
    except RuntimeError as e:
        print(e)


# ===============================================

def apply_snv(spectra):
    """ã€è®ºæ–‡ä¼˜åŒ–ã€‘æ ‡å‡†æ­£æ€å˜é‡å˜æ¢ (SNV)"""
    mean = np.mean(spectra, axis=1, keepdims=True)
    std = np.std(spectra, axis=1, keepdims=True)
    std[std == 0] = 1e-6
    return (spectra - mean) / std


def load_calibration_file(hdr_path):
    base = os.path.splitext(hdr_path)[0]
    spe = base + ".spe"
    if not os.path.exists(spe) and os.path.exists(base): spe = base
    img = envi.open(hdr_path, spe).load()
    if img.shape[1] == 208 and img.shape[2] != 208:
        img = np.transpose(img, (0, 2, 1))
    return np.array(img, dtype=np.float32)


def get_mask_from_json(json_path, img_shape):
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    mask = np.zeros(img_shape, dtype=np.uint8)
    labels_found = []
    for shape in data['shapes']:
        lbl = shape['label'].lower()
        labels_found.append(lbl)
        pts = np.array(shape['points'], dtype=np.int32)
        if 'no_pet' in lbl:
            cv2.fillPoly(mask, [pts], 2)  # NO_PET
        elif 'pet' in lbl:
            cv2.fillPoly(mask, [pts], 1)  # PET

    # è°ƒè¯•ä¿¡æ¯ï¼šå¦‚æœæ²¡æ‰¾åˆ° Maskï¼Œæ‰“å°ä¸€ä¸‹ JSON é‡Œçš„æ ‡ç­¾
    if np.sum(mask) == 0:
        # print(f"  âš ï¸ Warning: {os.path.basename(json_path)} ä¸­æœªåŒ¹é…åˆ° 'pet'/'no_pet'ã€‚åŒ…å«æ ‡ç­¾: {list(set(labels_found))}")
        pass
    return mask


# ä¿®æ”¹ main.py

def load_representative_data_for_drl():
    print("ğŸš€ [DRL] æ­£åœ¨åŠ è½½æ··åˆæ ·æœ¬ (PET + å¼ºè´Ÿæ ·æœ¬PP)...")

    # å®šä¹‰ä¸¤ä¸ªæ•°æ®æºï¼ˆå’Œ save_data.py ç±»ä¼¼ï¼‰
    dataset_configs = [
        # 1. PET æ–‡ä»¶å¤¹
        {
            "root": r"L:\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\12.12æ•°æ®é›†ï¼ˆå•æ’å…‰æºï¼‰\train-PET",
            "json_subdir": "fake_images",
            "is_pet_folder": True
        },
        # 2. Non-PET æ–‡ä»¶å¤¹ (PP, CCç­‰)
        {
            "root": r"I:\Hyperspectral Camera Dataset\Train_Data\no_PET\no_PET(CCé†‹é…¸çº¤ç»´ç´ )",
            "json_subdir": "fake_images",
            "is_pet_folder": False
        }
    ]

    white = load_calibration_file(WHITE_REF_HDR)
    dark = load_calibration_file(DARK_REF_HDR)
    denom = (white - dark)
    denom[denom == 0] = 1e-6

    collected_X = []
    collected_y = []

    # è®¡æ•°å™¨
    count_pet = 0
    count_hard_neg = 0  # PP, CC
    count_soft_neg = 0  # èƒŒæ™¯

    # ç›®æ ‡é‡‡æ ·æ•° (æ ¹æ®ä½ çš„æ˜¾å­˜è°ƒæ•´ï¼Œå»ºè®® 5000-10000)
    TARGET_PER_CLASS = 3000

    for config in dataset_configs:
        root_dir = config["root"]
        json_subdir = config["json_subdir"]
        is_pet_source = config["is_pet_folder"]

        if not os.path.exists(root_dir): continue

        files = [f for f in os.listdir(root_dir) if f.endswith('.spe')]

        for fname in files:
            # æå‰åœæ­¢æ¡ä»¶
            if is_pet_source and count_pet >= TARGET_PER_CLASS and count_soft_neg >= TARGET_PER_CLASS: continue
            if not is_pet_source and count_hard_neg >= TARGET_PER_CLASS: continue

            try:
                # ... (åŠ è½½ hdr, spe, æ ¡å‡† ä»£ç çœç•¥ï¼Œä¸ä¹‹å‰ä¸€è‡´) ...
                # å‡è®¾å¾—åˆ°äº† calib æ•°æ®

                # è·å– JSON è·¯å¾„
                base_name = os.path.splitext(fname)[0]
                json_path = os.path.join(root_dir, json_subdir, base_name + ".json")
                if not os.path.exists(json_path):
                    json_path = os.path.join(root_dir, base_name + ".json")

                # è§£æ Mask
                mask = get_mask_from_json(json_path, (calib.shape[0], calib.shape[1]))
                if mask is None: continue  # æˆ–è€…æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ Mask

                # æå–æ•°æ®
                flat_data = calib.reshape(-1, calib.shape[2])
                flat_mask = mask.reshape(-1)

                # --- æ ¸å¿ƒé€»è¾‘ï¼šåŒºåˆ†ä¸‰ç±» ---
                # 1. PET (æ ‡ç­¾ 1)
                idx_pet = np.where(flat_mask == 1)[0]
                # 2. å¼ºè´Ÿæ ·æœ¬ (æ ‡ç­¾ 2: PP/CC)
                idx_mat = np.where(flat_mask == 2)[0]
                # 3. å¼±è´Ÿæ ·æœ¬ (æ ‡ç­¾ 0: èƒŒæ™¯)
                idx_bg = np.where(flat_mask == 0)[0]

                # é‡‡æ ·å¹¶æ·»åŠ 
                # PET -> y=1
                if is_pet_source and len(idx_pet) > 0 and count_pet < TARGET_PER_CLASS:
                    take = min(len(idx_pet), 200)
                    sel = np.random.choice(idx_pet, take, replace=False)
                    collected_X.append(flat_data[sel])
                    collected_y.append(np.ones(take))  # y=1
                    count_pet += take

                # PP/CC -> y=0 (å…³é”®ï¼å‘Šè¯‰ DRL è¿™äº›ä¸æ˜¯ PET)
                if len(idx_mat) > 0 and count_hard_neg < TARGET_PER_CLASS:
                    take = min(len(idx_mat), 200)
                    sel = np.random.choice(idx_mat, take, replace=False)
                    collected_X.append(flat_data[sel])
                    collected_y.append(np.zeros(take))  # y=0
                    count_hard_neg += take

                # èƒŒæ™¯ -> y=0 (åªéœ€è¦å°‘é‡ï¼Œå‘Šè¯‰ DRL åŒºåˆ†èƒŒæ™¯)
                if is_pet_source and len(idx_bg) > 0 and count_soft_neg < TARGET_PER_CLASS:
                    take = min(len(idx_bg), 100)  # èƒŒæ™¯å°‘é‡‡ç‚¹ï¼Œå¾ˆå®¹æ˜“åŒºåˆ†
                    sel = np.random.choice(idx_bg, take, replace=False)
                    collected_X.append(flat_data[sel])
                    collected_y.append(np.zeros(take))  # y=0
                    count_soft_neg += take

            except:
                pass

    if not collected_X: raise ValueError("æ²¡æœ‰åŠ è½½åˆ°æ•°æ®")

    X = np.concatenate(collected_X, axis=0)
    y = np.concatenate(collected_y, axis=0)

    print(f"âœ… DRL é‡‡æ ·å®Œæˆ: PET={np.sum(y == 1)}, Non-PET(PP+BG)={np.sum(y == 0)}")
    return X, y

def train_dqn():
    # 1. å‡†å¤‡æ•°æ®
    X_full, y_full = load_representative_data_for_drl()

    # ã€è®ºæ–‡ä¼˜åŒ–ã€‘åº”ç”¨ SNV é¢„å¤„ç†
    print("ğŸ§ª æ­£åœ¨åº”ç”¨ SNV é¢„å¤„ç† (Paper Optimization)...")
    X_full = apply_snv(X_full)

    num_total_bands = X_full.shape[1]

    # 2. è®¡ç®—æŒ‡æ ‡
    print("ğŸ“Š æ­£åœ¨è®¡ç®— Information Entropy (ç†µ)...")
    entropies = precompute_entropies(X_full)

    print("âš–ï¸ æ­£åœ¨è®¡ç®— Mutual Information (äº’ä¿¡æ¯)...")
    mi_scores = precompute_mutual_information(X_full, y_full)

    # å½’ä¸€åŒ–
    entropies = (entropies - np.min(entropies)) / (np.max(entropies) - np.min(entropies) + 1e-6)
    mi_scores = (mi_scores - np.min(mi_scores)) / (np.max(mi_scores) - np.min(mi_scores) + 1e-6)

    # 3. è®­ç»ƒ Agent
    agent = BandSelectionAgent(num_total_bands)

    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ DRL Agent (Alpha={ALPHA})...")

    # ================= å…³é”®ä¿®å¤ï¼šåˆå§‹åŒ–å˜é‡ =================
    best_reward = -float('inf')
    best_bands = []  # ğŸ‘ˆ ä¹‹å‰æŠ¥é”™å°±æ˜¯å› ä¸ºå°‘äº†è¿™ä¸€è¡Œï¼
    # ======================================================

    for e in range(TOTAL_EPISODES):
        state = np.zeros(num_total_bands)
        selected_bands = []
        episode_reward = 0

        for step in range(NUM_BANDS_TO_SELECT):
            action = agent.get_action(state, selected_bands)
            reward = calculate_reward(selected_bands, action, entropies, mi_scores, alpha=ALPHA)

            next_state = state.copy()
            next_state[action] = 1
            done = (len(selected_bands) == NUM_BANDS_TO_SELECT - 1)

            agent.remember(state, action, reward, next_state, done)
            agent.train()

            state = next_state
            selected_bands.append(action)
            episode_reward += reward

        agent.update_target_network()
        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay

        # æ›´æ–°æœ€ä½³ç»“æœ
        if episode_reward > best_reward:
            best_reward = episode_reward
            best_bands = sorted(selected_bands)

        if (e + 1) % 10 == 0:
            print(
                f"Episode {e + 1}/{TOTAL_EPISODES} | Reward: {episode_reward:.4f} | Epsilon: {agent.epsilon:.2f} | Best: {len(best_bands)} bands")

    print("\n" + "=" * 50)
    print(f"ğŸ† æœ€ç»ˆæ¨èçš„ {len(best_bands)} ä¸ªæ³¢æ®µ (ç´¢å¼•):")
    print(best_bands)
    print("=" * 50)

    return best_bands


if __name__ == "__main__":
    final_bands = train_dqn()

    if not final_bands:
        print("âš ï¸ è­¦å‘Šï¼šè®­ç»ƒæœªè¿”å›æ³¢æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚")
        final_bands = list(range(30))

    # ä¿å­˜ç»“æœç»™ pipeline ä½¿ç”¨
    config_path = "best_bands_config.json"
    save_data = {"selected_bands": [int(b) for b in final_bands]}

    with open(config_path, "w") as f:
        json.dump(save_data, f)

    print(f"ğŸ’¾ [Auto] é…ç½®å·²ä¿å­˜è‡³ {config_path}")