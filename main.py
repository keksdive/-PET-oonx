import numpy as np
import os
import json
import tensorflow as tf
from sklearn.model_selection import train_test_split

# å¼•ç”¨ä½ çš„æ¨¡å—
from entropy_utils import precompute_entropies, precompute_mutual_information
from agent import BandSelectionAgent
from reward_utils import calculate_reward

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
# [é‡è¦] è¿™é‡ŒæŒ‡å‘ save_data.py ç”Ÿæˆçš„ .npy æ–‡ä»¶å¤¹
DATA_DIR = r"E:\SPEDATA\NP_newdata"

# [é…ç½®] è¾“å‡ºçš„æ³¢æ®µæ•°é‡
NUM_BANDS_TO_SELECT = 30

# [é…ç½®] è®­ç»ƒè½®æ•°
TOTAL_EPISODES = 300
ALPHA = 0.8  # äº’ä¿¡æ¯æƒé‡

# ===============================================

# æ˜¾å­˜é…ç½®
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
        print("âœ… GPU æ˜¾å­˜æŒ‰éœ€åˆ†é…å·²å¼€å¯")
    except RuntimeError as e:
        print(e)


def load_cleaned_data_for_drl():
    """
    ç›´æ¥åŠ è½½æ¸…æ´—åçš„ .npy æ•°æ® (X.npy, y.npy)
    """
    print(f"ğŸš€ [DRL] æ­£åœ¨åŠ è½½æ¸…æ´—åçš„æ•°æ®é›†: {DATA_DIR}")

    x_path = os.path.join(DATA_DIR, "X.npy")
    y_path = os.path.join(DATA_DIR, "y.npy")

    if not os.path.exists(x_path) or not os.path.exists(y_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° X.npy æˆ– y.npyï¼Œè¯·å…ˆè¿è¡Œ save_data.pyï¼è·¯å¾„: {DATA_DIR}")

    # 1. åŠ è½½æ•°æ®
    X = np.load(x_path).astype(np.float32)
    y = np.load(y_path).astype(np.float32)

    # 2. æ£€æŸ¥æ•°æ®
    # æˆ‘ä»¬ä¸éœ€è¦èƒŒæ™¯(0)ï¼Œä¹Ÿä¸éœ€è¦å¤ªå¤šçš„æ ·æœ¬å¯¼è‡´è®¡ç®—å¤ªæ…¢
    # save_data.py ç”Ÿæˆçš„æ•°æ®å·²ç»æ˜¯çº¯å‡€çš„æè´¨æ•°æ®äº†

    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {X.shape}")
    print(f"   æè´¨æ ‡ç­¾åˆ†å¸ƒ: {np.unique(y, return_counts=True)}")

    # 3. é‡‡æ · (å¦‚æœæ•°æ®é‡å¤ªå¤§ï¼Œæ¯”å¦‚ > 5ä¸‡ï¼ŒDRLè®¡ç®—äº’ä¿¡æ¯ä¼šå¾ˆæ…¢ï¼Œå»ºè®®é‡‡æ ·)
    MAX_SAMPLES = 20000
    if X.shape[0] > MAX_SAMPLES:
        print(f"âš ï¸ æ•°æ®é‡è¿‡å¤§ ({X.shape[0]}), éšæœºé‡‡æ · {MAX_SAMPLES} æ¡ç”¨äºç‰¹å¾é€‰æ‹©...")
        indices = np.random.choice(X.shape[0], MAX_SAMPLES, replace=False)
        X = X[indices]
        y = y[indices]

    return X, y


def train_dqn():
    # 1. åŠ è½½æ•°æ® (å·²æ¸…æ´—ã€å·²å½’ä¸€åŒ–)
    X_full, y_full = load_cleaned_data_for_drl()

    # âš ï¸ æ³¨æ„ï¼šå› ä¸º save_data.py å·²ç»åšäº† Min-Max å½’ä¸€åŒ–ï¼Œ
    # è¿™é‡Œä¸éœ€è¦å†åš SNV æˆ–å…¶ä»–å½’ä¸€åŒ–ï¼Œä¿æŒå’Œè®­ç»ƒæ—¶ä¸€è‡´å³å¯ã€‚
    # å¦‚æœä½  save_data.py æ²¡åšå½’ä¸€åŒ–ï¼Œè¿™é‡Œæ‰éœ€è¦åšã€‚
    # å‡è®¾ä½ ç”¨çš„æ˜¯æˆ‘åˆšæ‰ç»™çš„ save_data.py (å« Min-Max)ï¼Œè¿™é‡Œç›´æ¥ç”¨ã€‚

    # è£å‰ªå¼‚å¸¸å€¼ (Double check)
    X_full = np.clip(X_full, 0, 1)

    num_total_bands = X_full.shape[1]
    print(f"ğŸ“Š æ€»æ³¢æ®µæ•°: {num_total_bands}")

    # 2. è®¡ç®—æŒ‡æ ‡
    print("âš–ï¸ è®¡ç®—äº’ä¿¡æ¯ (Mutual Information)...")
    # è¿™é‡Œçš„ y_full åŒ…å« 1(PET), 2(CC), 3(PA) ç­‰
    # äº’ä¿¡æ¯ä¼šè‡ªåŠ¨è®¡ç®—æ³¢æ®µä¸è¿™äº›ç±»åˆ«çš„ç›¸å…³æ€§
    mi_scores = precompute_mutual_information(X_full, y_full)

    print("ğŸ“‰ è®¡ç®—ç†µ (Entropy)...")
    entropies = precompute_entropies(X_full)

    # å½’ä¸€åŒ–æŒ‡æ ‡åˆ° 0-1
    mi_scores = (mi_scores - np.min(mi_scores)) / (np.max(mi_scores) - np.min(mi_scores) + 1e-6)
    entropies = (entropies - np.min(entropies)) / (np.max(entropies) - np.min(entropies) + 1e-6)

    # 3. è®­ç»ƒ Agent
    agent = BandSelectionAgent(num_total_bands)

    print(f"\nğŸ”¥ å¼€å§‹ç­›é€‰ç‰¹å¾æ³¢æ®µ (ç›®æ ‡: {NUM_BANDS_TO_SELECT}ä¸ª)...")

    best_reward = -float('inf')
    best_bands = []

    for e in range(TOTAL_EPISODES):
        state = np.zeros(num_total_bands)
        selected_bands = []
        episode_reward = 0

        for step in range(NUM_BANDS_TO_SELECT):
            action = agent.get_action(state, selected_bands)

            # è®¡ç®—å¥–åŠ±
            reward = calculate_reward(selected_bands, action, entropies, mi_scores, alpha=ALPHA)

            next_state = state.copy()
            next_state[action] = 1
            done = (len(selected_bands) == NUM_BANDS_TO_SELECT - 1)

            agent.remember(state, action, reward, next_state, done)
            agent.train()

            state = next_state
            selected_bands.append(action)
            episode_reward += reward

        agent.update_target_network()
        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay

        if episode_reward > best_reward:
            best_reward = episode_reward
            best_bands = sorted(selected_bands)

        if (e + 1) % 10 == 0:
            print(f"Episode {e + 1}/{TOTAL_EPISODES} | Reward: {episode_reward:.2f} | Epsilon: {agent.epsilon:.2f}")

    print(f"\nğŸ† ç­›é€‰å®Œæˆã€‚å…±ç­›é€‰ {len(best_bands)} ä¸ªæè´¨ç‰¹å¾æ³¢æ®µ:\n{best_bands}")
    return best_bands


if __name__ == "__main__":
    final_bands = train_dqn()

    if not final_bands:
        print("âš ï¸ ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ³¢æ®µ")
        final_bands = list(range(30))

    output_filename = "best_bands_config.json"
    save_data = {
        "description": "Selected using Cleaned Normalized Data (X.npy)",
        "count": len(final_bands),
        "selected_bands": [int(b) for b in final_bands]
    }

    with open(output_filename, "w") as f:
        json.dump(save_data, f, indent=4)

    print(f"ğŸ’¾ é…ç½®æ–‡ä»¶å·²æ›´æ–°: {os.path.abspath(output_filename)}")