import numpy as np
import os
import json
import tensorflow as tf
from sklearn.model_selection import train_test_split
from agent import BandSelectionAgent
from reward_utils import calculate_reward_supervised  # ç¡®ä¿è¿™é‡Œå¼•ç”¨çš„æ˜¯ä¿®æ”¹åçš„ k-NN ç‰ˆæœ¬

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
DATA_DIR = r"E:\SPEDATA\NP_new1.0.2"  # æŒ‡å‘ä½ æ–°ç”Ÿæˆçš„æ•°æ®è·¯å¾„
NUM_BANDS_TO_SELECT = 30
TOTAL_EPISODES = 500

# DRL ä¸“ç”¨æ•°æ®é›†å¤§å° (æ¯ç±»æ ·æœ¬æ•°)
# å»ºè®®ï¼šæ¯ç±» 2500ï¼Œæ€»å…± 5000ã€‚å¤ªå¤§ä¼šå¯¼è‡´ k-NN è®¡ç®—å¥–åŠ±å˜æ…¢ã€‚
SAMPLES_PER_CLASS = 2500
# ===============================================

# æ˜¾å­˜é…ç½®
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
    except:
        pass


def load_data():
    x_path = os.path.join(DATA_DIR, "X.npy")
    y_path = os.path.join(DATA_DIR, "y.npy")
    if not os.path.exists(x_path): raise Exception(f"Data not found in {DATA_DIR}")

    # ä½¿ç”¨ mmap_mode='r' å¯ä»¥é¿å…ä¸€æ¬¡æ€§æŠŠ 40ä¸‡æ•°æ®è¯»å…¥å†…å­˜ï¼ŒèŠ‚çœå†…å­˜
    X = np.load(x_path, mmap_mode='r')
    y = np.load(y_path)
    return X, y


def prepare_balanced_drl_data(X_full, y_full, samples_per_class=2000):
    """
    [æ–°å¢] æ„é€ ä¸€ä¸ªä¸¥æ ¼å¹³è¡¡çš„ (1:1) å°è§„æ¨¡æ•°æ®é›†ç”¨äº DRL å¥–åŠ±è®¡ç®—
    """
    print(f"âš–ï¸ æ­£åœ¨å¹³è¡¡æ•°æ®é›† (ç›®æ ‡: æ¯ç±» {samples_per_class} ä¸ª)...")

    # 1. æ‰¾å‡ºæ­£è´Ÿæ ·æœ¬ç´¢å¼•
    idx_pos = np.where(y_full == 1)[0]
    idx_neg = np.where(y_full == 0)[0]

    print(f"   - åŸå§‹æ­£æ ·æœ¬æ•°: {len(idx_pos)}")
    print(f"   - åŸå§‹è´Ÿæ ·æœ¬æ•°: {len(idx_neg)}")

    # 2. æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
    real_samples = min(len(idx_pos), len(idx_neg), samples_per_class)

    # 3. éšæœºæŠ½å– (æ— æ”¾å›)
    # æ³¨æ„ï¼šå› ä¸º X æ˜¯ mmapï¼Œè¿™é‡Œåªæ“ä½œç´¢å¼•
    selected_pos = np.random.choice(idx_pos, real_samples, replace=False)
    selected_neg = np.random.choice(idx_neg, real_samples, replace=False)

    # 4. åˆå¹¶ç´¢å¼•
    selected_indices = np.concatenate([selected_pos, selected_neg])

    # 5. [å…³é”®] å¿…é¡»æ‰“ä¹±ï¼Œå¦åˆ™å‰é¢å…¨æ˜¯1åé¢å…¨æ˜¯0
    np.random.shuffle(selected_indices)

    # 6. çœŸæ­£åŠ è½½æ•°æ®åˆ°å†…å­˜
    # åªæœ‰è¿™ä¸€æ­¥æ‰ä¼šæŠŠæ•°æ®è¯»å…¥ RAM
    X_balanced = X_full[selected_indices].astype(np.float32)
    y_balanced = y_full[selected_indices].astype(np.float32)

    print(f"âœ… å¹³è¡¡å®Œæˆ: æ€»æ•° {len(y_balanced)}, æ­£è´Ÿæ¯” 1:1")
    return X_balanced, y_balanced


def train_dqn():
    # 1. åŠ è½½å…¨é‡æ•°æ® (Lazy Load)
    X_full, y_full = load_data()
    num_total_bands = X_full.shape[1]

    # 2. [ä¿®æ”¹] è·å–å¹³è¡¡çš„ DRL ä¸“ç”¨æ•°æ®é›†
    X_drl, y_drl = prepare_balanced_drl_data(X_full, y_full, SAMPLES_PER_CLASS)

    # 3. å†æ¬¡åˆ’åˆ†ä¸º k-NN çš„ è®­ç»ƒé›† (Fit) å’Œ éªŒè¯é›† (Score)
    # è¿™é‡Œä¸éœ€è¦å† stratifyï¼Œå› ä¸ºå·²ç»æ˜¯ 1:1 äº†ï¼Œæ™®é€š shuffle split å³å¯
    X_reward_train, X_reward_val, y_reward_train, y_reward_val = train_test_split(
        X_drl, y_drl, test_size=0.4, random_state=42
    )

    print(f"ğŸ“Š DRL å¥–åŠ±è®¡ç®—é›† (ç”¨äº k-NN):")
    print(f"   - Fit Set  : {X_reward_train.shape} (ç”¨äºæ„å»ºåˆ†ç±»å™¨)")
    print(f"   - Val Set  : {X_reward_val.shape} (ç”¨äºè®¡ç®— OA)")

    # 4. åˆå§‹åŒ– Agent
    agent = BandSelectionAgent(num_total_bands)
    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ D3QN-SBS (ç›®æ ‡: {NUM_BANDS_TO_SELECT} æ³¢æ®µ)...")

    best_reward = -float('inf')
    best_bands = []

    for e in range(TOTAL_EPISODES):
        state = np.zeros(num_total_bands)  # åˆå§‹çŠ¶æ€
        selected_bands = []
        episode_reward = 0

        for step in range(NUM_BANDS_TO_SELECT):
            # è·å–åŠ¨ä½œ
            action = agent.get_action(state, selected_bands)

            # è®¡ç®—å¥–åŠ± (ä½¿ç”¨å¹³è¡¡æ•°æ®é›†è®¡ç®— OA)
            reward = calculate_reward_supervised(
                selected_bands, action,
                X_reward_train, y_reward_train,
                X_reward_val, y_reward_val
            )

            # æ›´æ–°çŠ¶æ€
            next_state = state.copy()
            next_state[action] = 1

            done = (len(selected_bands) == NUM_BANDS_TO_SELECT - 1)

            agent.remember(state, action, reward, next_state, done)
            agent.train()

            state = next_state
            selected_bands.append(action)
            episode_reward += reward

        agent.update_target_network()

        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay

        # è®°å½•æœ€ä½³
        if episode_reward > best_reward:
            best_reward = episode_reward
            best_bands = sorted(selected_bands)
            print(f"ğŸŒŸ [New Best] Ep {e + 1} | Reward: {episode_reward:.4f} | Bands: {best_bands}")

        if (e + 1) % 10 == 0:
            print(f"Episode {e + 1}/{TOTAL_EPISODES} | Reward: {episode_reward:.4f} | Epsilon: {agent.epsilon:.2f}")

    print(f"\nğŸ† æœ€ç»ˆç­›é€‰ç»“æœ: {best_bands}")
    return best_bands


if __name__ == "__main__":
    final_bands = train_dqn()

    # ä¿å­˜ç»“æœ
    output_filename = "best_bands_config.json"
    save_data = {
        "description": "D3QN-SBS (k-NN Reward, Balanced Data)",
        "count": len(final_bands),
        "selected_bands": [int(b) for b in final_bands]
    }
    with open(output_filename, "w") as f:
        json.dump(save_data, f, indent=4)
    print(f"ğŸ’¾ é…ç½®æ–‡ä»¶å·²ä¿å­˜: {output_filename}")