import numpy as np
import os
import json
import tensorflow as tf
from sklearn.model_selection import train_test_split
from agent import BandSelectionAgent
from reward_utils import calculate_reward_supervised  # ç¡®ä¿è¿™é‡Œå¼•ç”¨çš„æ˜¯ä¿®æ”¹åçš„ k-NN ç‰ˆæœ¬
import datetime

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
DATA_DIR = r"E:\SPEDATA\NP_new1.0.2"  # æŒ‡å‘ä½ æ–°ç”Ÿæˆçš„æ•°æ®è·¯å¾„
NUM_BANDS_TO_SELECT = 30
TOTAL_EPISODES = 500

# è‡ªåŠ¨ä¿å­˜è·¯å¾„
AUTO_SAVE_DIR = r"D:\best-bands"
if not os.path.exists(AUTO_SAVE_DIR):
    os.makedirs(AUTO_SAVE_DIR)

# DRL ä¸“ç”¨æ•°æ®é›†å¤§å° (æ¯ç±»æ ·æœ¬æ•°)
# å»ºè®®ï¼šæ¯ç±» 2500ï¼Œæ€»å…± 5000ã€‚å¤ªå¤§ä¼šå¯¼è‡´ k-NN è®¡ç®—å¥–åŠ±å˜æ…¢ã€‚
SAMPLES_PER_CLASS = 2500
# ===============================================

# æ˜¾å­˜é…ç½®
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
    except:
        pass


def load_data():
    x_path = os.path.join(DATA_DIR, "X.npy")
    y_path = os.path.join(DATA_DIR, "y.npy")
    if not os.path.exists(x_path): raise Exception(f"Data not found in {DATA_DIR}")

    # ä½¿ç”¨ mmap_mode='r' å¯ä»¥é¿å…ä¸€æ¬¡æ€§æŠŠ 40ä¸‡æ•°æ®è¯»å…¥å†…å­˜ï¼ŒèŠ‚çœå†…å­˜
    X = np.load(x_path, mmap_mode='r')
    y = np.load(y_path)
    return X, y


def prepare_balanced_drl_data(X_full, y_full, samples_per_class=2000):
    """
    [æ–°å¢] æ„é€ ä¸€ä¸ªä¸¥æ ¼å¹³è¡¡çš„ (1:1) å°è§„æ¨¡æ•°æ®é›†ç”¨äº DRL å¥–åŠ±è®¡ç®—
    """
    print(f"âš–ï¸ æ­£åœ¨å¹³è¡¡æ•°æ®é›† (ç›®æ ‡: æ¯ç±» {samples_per_class} ä¸ª)...")

    # 1. æ‰¾å‡ºæ­£è´Ÿæ ·æœ¬ç´¢å¼•
    idx_pos = np.where(y_full == 1)[0]
    idx_neg = np.where(y_full == 0)[0]

    print(f"   - åŸå§‹æ­£æ ·æœ¬æ•°: {len(idx_pos)}")
    print(f"   - åŸå§‹è´Ÿæ ·æœ¬æ•°: {len(idx_neg)}")

    # 2. æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
    real_samples = min(len(idx_pos), len(idx_neg), samples_per_class)

    # 3. éšæœºæŠ½å– (æ— æ”¾å›)
    # æ³¨æ„ï¼šå› ä¸º X æ˜¯ mmapï¼Œè¿™é‡Œåªæ“ä½œç´¢å¼•
    selected_pos = np.random.choice(idx_pos, real_samples, replace=False)
    selected_neg = np.random.choice(idx_neg, real_samples, replace=False)

    # 4. åˆå¹¶ç´¢å¼•
    selected_indices = np.concatenate([selected_pos, selected_neg])

    # 5. [å…³é”®] å¿…é¡»æ‰“ä¹±ï¼Œå¦åˆ™å‰é¢å…¨æ˜¯1åé¢å…¨æ˜¯0
    np.random.shuffle(selected_indices)

    # 6. çœŸæ­£åŠ è½½æ•°æ®åˆ°å†…å­˜
    # åªæœ‰è¿™ä¸€æ­¥æ‰ä¼šæŠŠæ•°æ®è¯»å…¥ RAM
    X_balanced = X_full[selected_indices].astype(np.float32)
    y_balanced = y_full[selected_indices].astype(np.float32)

    print(f"âœ… å¹³è¡¡å®Œæˆ: æ€»æ•° {len(y_balanced)}, æ­£è´Ÿæ¯” 1:1")
    return X_balanced, y_balanced


def save_best_bands(bands, epsilon, reward):
    """
    ä¿å­˜å½“å‰çš„ç‰¹å¾æ³¢æ®µé…ç½®
    æ–‡ä»¶åæ ¼å¼ï¼šä¿å­˜æ—¶é—´-Epsilon-Reward.json
    """
    time_str = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    # æ ¼å¼åŒ–æ–‡ä»¶åï¼š20260122-163000-Eps0.80.json
    filename = f"{time_str}-Eps{epsilon:.4f}.json"
    save_path = os.path.join(AUTO_SAVE_DIR, filename)

    save_data = {
        "description": f"Auto-saved at Epsilon {epsilon:.4f}, Reward {reward:.4f}",
        "timestamp": time_str,
        "epsilon": epsilon,
        "reward": reward,
        "count": len(bands),
        "selected_bands": [int(b) for b in bands]
    }

    with open(save_path, "w") as f:
        json.dump(save_data, f, indent=4)

    print(f"ğŸ’¾ [Auto Save] å·²ä¿å­˜æ³¢æ®µé…ç½®: {filename}")


def train_dqn():
    # 1. åŠ è½½å…¨é‡æ•°æ® (Lazy Load)
    X_full, y_full = load_data()
    num_total_bands = X_full.shape[1]

    # 2. [ä¿®æ”¹] è·å–å¹³è¡¡çš„ DRL ä¸“ç”¨æ•°æ®é›†
    X_drl, y_drl = prepare_balanced_drl_data(X_full, y_full, SAMPLES_PER_CLASS)

    # 3. å†æ¬¡åˆ’åˆ†ä¸º k-NN çš„ è®­ç»ƒé›† (Fit) å’Œ éªŒè¯é›† (Score)
    # è¿™é‡Œä¸éœ€è¦å† stratifyï¼Œå› ä¸ºå·²ç»æ˜¯ 1:1 äº†ï¼Œæ™®é€š shuffle split å³å¯
    X_reward_train, X_reward_val, y_reward_train, y_reward_val = train_test_split(
        X_drl, y_drl, test_size=0.4, random_state=42
    )

    print(f"ğŸ“Š DRL å¥–åŠ±è®¡ç®—é›† (ç”¨äº k-NN):")
    print(f"   - Fit Set  : {X_reward_train.shape} (ç”¨äºæ„å»ºåˆ†ç±»å™¨)")
    print(f"   - Val Set  : {X_reward_val.shape} (ç”¨äºè®¡ç®— OA)")

    # 4. åˆå§‹åŒ– Agent
    agent = BandSelectionAgent(num_total_bands)
    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ D3QN-SBS (ç›®æ ‡: {NUM_BANDS_TO_SELECT} æ³¢æ®µ)...")

    best_reward = -float('inf')
    best_bands = []

    # === è‡ªåŠ¨ä¿å­˜é€»è¾‘åˆå§‹åŒ– ===
    # åˆå§‹é˜ˆå€¼è®¾ä¸º 0.8 (å› ä¸ºåˆå§‹ epsilon æ˜¯ 1.0ï¼Œç¬¬ä¸€æ¬¡ä¸‹é™ 0.2 åä¿å­˜)
    next_save_threshold = 0.8

    for e in range(TOTAL_EPISODES):
        state = np.zeros(num_total_bands)  # åˆå§‹çŠ¶æ€
        selected_bands = []
        episode_reward = 0

        for step in range(NUM_BANDS_TO_SELECT):
            # è·å–åŠ¨ä½œ
            action = agent.get_action(state, selected_bands)

            # è®¡ç®—å¥–åŠ± (ä½¿ç”¨å¹³è¡¡æ•°æ®é›†è®¡ç®— OA)
            reward = calculate_reward_supervised(
                selected_bands, action,
                X_reward_train, y_reward_train,
                X_reward_val, y_reward_val
            )

            # æ›´æ–°çŠ¶æ€
            next_state = state.copy()
            next_state[action] = 1

            done = (len(selected_bands) == NUM_BANDS_TO_SELECT - 1)

            agent.remember(state, action, reward, next_state, done)
            agent.train()

            state = next_state
            selected_bands.append(action)
            episode_reward += reward

        agent.update_target_network()

        # Epsilon è¡°å‡
        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay

        # === è‡ªåŠ¨ä¿å­˜æ£€æŸ¥é€»è¾‘ ===
        # æ£€æŸ¥å½“å‰ Epsilon æ˜¯å¦è·Œç ´äº†ä¸‹ä¸€æ¬¡ä¿å­˜çš„é˜ˆå€¼
        if agent.epsilon <= next_save_threshold:
            # åªæœ‰å½“æ‰¾åˆ°äº†æœ‰æ•ˆæ³¢æ®µæ—¶æ‰ä¿å­˜
            if best_bands:
                save_best_bands(best_bands, agent.epsilon, best_reward)

            # æ›´æ–°ä¸‹ä¸€ä¸ªé˜ˆå€¼
            if agent.epsilon > 0.2:
                next_save_threshold -= 0.2  # é«˜æ¢ç´¢é˜¶æ®µï¼šæ¯é™ 0.2 ä¿å­˜
            else:
                next_save_threshold -= 0.01  # ä½æ¢ç´¢é˜¶æ®µï¼šæ¯é™ 0.01 ä¿å­˜

            # é˜²æ­¢é˜ˆå€¼å˜æˆè´Ÿæ•°ï¼ˆè™½ç„¶ epsilon_min æ˜¯ 0.01ï¼Œä½†é€»è¾‘ä¸Šä¿é™©ä¸€ç‚¹ï¼‰
            if next_save_threshold < 0:
                next_save_threshold = 0

        # è®°å½•æœ€ä½³
        if episode_reward > best_reward:
            best_reward = episode_reward
            best_bands = sorted(selected_bands)
            print(f"ğŸŒŸ [New Best] Ep {e + 1} | Reward: {episode_reward:.4f} | Bands: {best_bands}")

        if (e + 1) % 10 == 0:
            print(f"Episode {e + 1}/{TOTAL_EPISODES} | Reward: {episode_reward:.4f} | Epsilon: {agent.epsilon:.4f}")

    print(f"\nğŸ† æœ€ç»ˆç­›é€‰ç»“æœ: {best_bands}")
    return best_bands


if __name__ == "__main__":
    # 1. è¿è¡Œ DRL ç­›é€‰æ³¢æ®µ
    final_bands = train_dqn()

    # 2. é‡æ–°åŠ è½½å°‘é‡æ•°æ®ç”¨äºç»˜å›¾å±•ç¤º
    # ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„ load_data è·å– X_full å’Œ y_full
    X_full, y_full = load_data()
    # é‡‡æ ·ä¸€éƒ¨åˆ† PET æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    X_plot, y_plot = prepare_balanced_drl_data(X_full, y_full, samples_per_class=1000)

    # 3. [æ–°å¢åŠŸèƒ½] æ‰§è¡ŒéªŒè¯ä¸ç»˜å›¾
    # æ³¨æ„ï¼šç¡®ä¿ visualization.py ä¸­æœ‰è¿™ä¸ªå‡½æ•°
    try:
        from visualization import visualize_and_verify_pet_bands

        visualize_and_verify_pet_bands(
            X_data=X_plot,
            y_data=y_plot,
            selected_bands=final_bands,
            save_path="pet_feature_validation.png"
        )
    except ImportError:
        print("âš ï¸ æœªæ‰¾åˆ° visualize_and_verify_pet_bands å‡½æ•°ï¼Œè·³è¿‡ç»˜å›¾")

    # 4. ä¿å­˜ JSON é…ç½®æ–‡ä»¶ (æœ€ç»ˆç»“æœ)
    output_filename = "best_bands_config.json"
    save_data = {
        "description": "D3QN-SBS Final Result",
        "count": len(final_bands),
        "selected_bands": [int(b) for b in final_bands]
    }
    with open(output_filename, "w") as f:
        json.dump(save_data, f, indent=4)
    print(f"ğŸ’¾ æœ€ç»ˆé…ç½®æ–‡ä»¶å·²ä¿å­˜: {output_filename}")