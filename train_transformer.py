import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, mixed_precision
import os
import json
import glob
import random

# ================= ğŸ”§ é…ç½® =================
CONFIG_FILE = "best_bands_config.json"
DATA_ROOT = r"E:\SPEDATA\NP_data"  # è¯·ç¡®è®¤æ­¤è·¯å¾„ä¸ save_data.py ä¸€è‡´
MODEL_SAVE_PATH = r"D:\DRL\DRL1\final_model.h5"

BATCH_SIZE = 64
EPOCHS = 50
PIXELS_PER_FILE = 1000  # å¢åŠ é‡‡æ ·ç‚¹æ•°ä»¥æé«˜è¦†ç›–ç‡

# ç±»åˆ«æ˜ å°„ (å¯¹åº” save_data.py çš„æ–‡ä»¶å¤¹)
# å¿…é¡»ä¸ main.py çš„é€»è¾‘ä¿æŒä¸€è‡´
CLASS_MAP = {"Background": 0, "PET": 1, "CC": 2, "PA": 3, "OTHER": 5}
NUM_CLASSES = len(CLASS_MAP)

# å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (å¦‚æœæ˜¾å¡æ”¯æŒï¼Œå¯å¤§å¹…åŠ é€Ÿ)
try:
    mixed_precision.set_global_policy('mixed_float16')
except:
    pass


# ================= ğŸ§  æ•°æ®ç”Ÿæˆå™¨ =================
class SpectralDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, file_list, selected_bands, batch_size=64, samples_per_file=500):
        self.file_list = file_list
        self.selected_bands = selected_bands  # è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«å»é‡åçš„æ‰€æœ‰ç‰¹å¾æ³¢æ®µç´¢å¼•
        self.batch_size = batch_size
        self.samples_per_file = samples_per_file
        self.indexes = np.arange(len(self.file_list))
        self.on_epoch_end()

    def __len__(self):
        return int(len(self.file_list) * 2)

    def on_epoch_end(self):
        np.random.shuffle(self.indexes)

    def __getitem__(self, index):
        batch_X, batch_y = [], []

        # åŠ¨æ€å°è¯•å¤šæ¬¡å¡«å……ç›´åˆ° Batch æ»¡
        attempts = 0
        while len(batch_X) < self.batch_size and attempts < 100:
            attempts += 1
            file_idx = np.random.choice(len(self.file_list))
            file_path = self.file_list[file_idx]

            # 1. ç¡®å®š Label
            folder_name = os.path.basename(os.path.dirname(file_path))
            # æ¨¡ç³ŠåŒ¹é…æ–‡ä»¶å¤¹å (ä¾‹å¦‚ "train-PET" -> "PET")
            label_id = 5  # Default OTHER
            for key, val in CLASS_MAP.items():
                if key in folder_name.upper():
                    label_id = val
                    break
            if label_id == 0:  # å¦‚æœè·¯å¾„æ²¡åŒ¹é…ä¸Šï¼Œä¹Ÿé»˜è®¤èƒŒæ™¯
                pass

            # 2. åŠ è½½æ•°æ®
            try:
                img = np.load(file_path).astype(np.float32)
            except:
                continue

            # 3. ç­›é€‰æ³¢æ®µ (æ ¸å¿ƒä¿®æ”¹ç‚¹)
            # ä½¿ç”¨ DRL é€‰å‡ºçš„ç‰¹å®šæ³¢æ®µç»„åˆ
            if self.selected_bands:
                img = img[:, :, self.selected_bands]

            # 4. å‰æ™¯èƒŒæ™¯åˆ†ç¦»é‡‡æ ·
            intensity = np.mean(img, axis=2)
            fg_mask = intensity > 0.05

            # å¦‚æœæ˜¯èƒŒæ™¯ç±»æ–‡ä»¶å¤¹ï¼Œæˆ–è€…æœ¬èº«å°±æ˜¯æš—åƒç´  -> Label 0
            if label_id == 0:
                target_pixels = img.reshape(-1, img.shape[-1])
                target_label = 0
            else:
                # å¦‚æœæ˜¯æè´¨æ–‡ä»¶å¤¹ï¼Œå–å‰æ™¯ -> Label IDï¼Œå–èƒŒæ™¯ -> Label 0
                fg_pixels = img[fg_mask]
                if len(fg_pixels) > 0:
                    take = min(len(fg_pixels), self.samples_per_file)
                    idx = np.random.choice(len(fg_pixels), take)
                    batch_X.append(fg_pixels[idx])
                    batch_y.append(np.full(take, label_id))
                continue  # èƒŒæ™¯éƒ¨åˆ†å·²éšå«åœ¨å…¶ä»–æ–‡ä»¶æˆ–é€šè¿‡ä½é˜ˆå€¼å¤„ç†

            if len(target_pixels) > 0:
                take = min(len(target_pixels), self.samples_per_file)
                idx = np.random.choice(len(target_pixels), take)
                batch_X.append(target_pixels[idx])
                batch_y.append(np.full(take, target_label))

        if len(batch_X) == 0:  # é˜²æ­¢ç©ºæ•°æ®æŠ¥é”™
            return np.zeros((self.batch_size, len(self.selected_bands))), np.zeros(self.batch_size)

        X_out = np.vstack(batch_X)
        y_out = np.concatenate(batch_y)

        # æˆªæ–­æˆ–å¡«å……
        if len(X_out) > self.batch_size:
            indices = np.random.choice(len(X_out), self.batch_size, replace=False)
            return X_out[indices], y_out[indices]
        else:
            # æ•°æ®ä¸è¶³æ—¶é‡å¤å¡«å……
            indices = np.random.choice(len(X_out), self.batch_size, replace=True)
            return X_out[indices], y_out[indices]


# ================= ğŸš€ æ¨¡å‹æ„å»º =================
def build_transformer_model(input_dim, num_classes):
    """
    æ„å»ºä¸€ä¸ªå¢å¼ºå‹åˆ†ç±»ç½‘ç»œï¼Œç»“åˆ 1D-CNN æå–å±€éƒ¨å…‰è°±ç‰¹å¾ å’Œ Transformer æå–å…¨å±€ç›¸å…³æ€§
    """
    inputs = layers.Input(shape=(input_dim,))
    x = layers.Reshape((input_dim, 1))(inputs)
    # CNN + Transformer Encoder ç»“æ„..

    # 2. å±€éƒ¨ç‰¹å¾æå– (CNN)
    x = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling1D(pool_size=2)(x)

    x = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling1D(pool_size=2)(x)

    # 3. å…¨å±€ç‰¹å¾æå– (Transformer Encoder Block)
    # Multi-Head Attention
    attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
    x = layers.Add()([x, attention_output])  # Residual
    x = layers.LayerNormalization(epsilon=1e-6)(x)

    # Feed Forward
    ffn = layers.Dense(128, activation="relu")(x)
    ffn = layers.Dense(128)(ffn)  # ä¿æŒç»´åº¦
    x = layers.Add()([x, ffn])  # Residual
    x = layers.LayerNormalization(epsilon=1e-6)(x)

    # 4. åˆ†ç±»å¤´
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.4)(x)

    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)  # ç¡®ä¿è¾“å‡ºä¸º float32

    model = models.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model


if __name__ == "__main__":
    with open(CONFIG_FILE, 'r') as f:
        config = json.load(f)
        # âš ï¸ å…³é”®ï¼šåˆå¹¶å»é‡åçš„æ³¢æ®µ
        bands = config.get("all_unique_bands", [])

    print(f"ğŸ¤– ä½¿ç”¨ç‰¹å¾æ³¢æ®µæ€»æ•°: {len(bands)}")
    print(f"   (åŒ…å« PETã€CCã€PA çš„å…³é”®ç‰¹å¾å¹¶é›†)")

    # 2. æ‰«ææ•°æ®
    all_files = glob.glob(os.path.join(DATA_ROOT, "**", "*.npy"), recursive=True)
    random.shuffle(all_files)

    split = int(len(all_files) * 0.8)
    train_files = all_files[:split]
    val_files = all_files[split:]

    print(f"ğŸ“‚ è®­ç»ƒé›†æ–‡ä»¶: {len(train_files)} | éªŒè¯é›†æ–‡ä»¶: {len(val_files)}")

    # 3. è®­ç»ƒ
    train_gen = SpectralDataGenerator(train_files, bands, batch_size=BATCH_SIZE)
    val_gen = SpectralDataGenerator(val_files, bands, batch_size=BATCH_SIZE)

    model = build_transformer_model(len(bands), NUM_CLASSES)
    model.summary()

    callbacks_list = [
        callbacks.ModelCheckpoint(MODEL_SAVE_PATH, save_best_only=True, monitor='val_accuracy'),
        callbacks.ReduceLROnPlateau(factor=0.5, patience=3),
        callbacks.EarlyStopping(patience=8, restore_best_weights=True)
    ]

    history = model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=EPOCHS,
        callbacks=callbacks_list
    )

    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {MODEL_SAVE_PATH}")