import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint
import os
import time
import datetime
import tf2onnx

# ================= 1. ç¡¬ä»¶æ£€æŸ¥ =================
gpus = tf.config.list_physical_devices('GPU')
print(f"\n{'=' * 40}")
if len(gpus) > 0:
    print(f"âœ… æ˜¾å¡å°±ç»ª: {gpus[0].name}")
    try:
        from tensorflow.keras import mixed_precision

        mixed_precision.set_global_policy('mixed_float16')
        print("âš¡ å·²å¯ç”¨æ··åˆç²¾åº¦ (Mixed Precision) åŠ é€Ÿ")
    except:
        pass
else:
    print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†åœ¨ CPU ä¸Šè¿è¡Œ Transformer (é€Ÿåº¦è¾ƒæ…¢)")
print(f"{'=' * 40}\n")

# ================= ğŸ”§ è·¯å¾„é…ç½® =================
DATA_DIR = r"E:\SPEDATA\NP_newdata"
MODEL_SAVE_DIR = r"D:\DRL\DRL1\models"
if not os.path.exists(MODEL_SAVE_DIR): os.makedirs(MODEL_SAVE_DIR)

# âš¡ å‚æ•°é…ç½®
BATCH_SIZE = 2048
EPOCHS = 100


# ================= 2. æ•°æ®ç®¡é“ =================
def create_dataset(X, y, is_training=True):
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    if is_training:
        dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.cache()
    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)
    return dataset


# ================= 3. Transformer æ¨¡å‹å®šä¹‰ (å¼ºåˆ¶ä½¿ç”¨) =================
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # 1. Normalization & Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # 2. Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_transformer_model(input_shape):
    """
    æ„å»º Hybrid CNN-Transformer æ¨¡å‹
    ç»“åˆ CNN çš„å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›å’Œ Transformer çš„å…¨å±€åºåˆ—å»ºæ¨¡èƒ½åŠ›
    """
    inputs = layers.Input(shape=input_shape)

    # å¢åŠ ä¸€ä¸ªç»´åº¦ä»¥é€‚é… Conv1D: (Batch, Bands) -> (Batch, Bands, 1)
    x = layers.Reshape((input_shape[0], 1))(inputs)

    # --- Feature Extraction (CNN) ---
    # å…ˆç”¨ CNN æå–æ³¢è°±çš„å±€éƒ¨ç‰¹å¾ï¼ˆæ³¢å³°/æ³¢è°·çš„æ–œç‡ç­‰ï¼‰
    x = layers.Conv1D(32, 5, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling1D(2)(x)

    x = layers.Conv1D(64, 3, padding="same", activation="relu")(x)
    x = layers.MaxPooling1D(2)(x)

    # --- Sequence Modeling (Transformer) ---
    # å¼ºåˆ¶ä½¿ç”¨ Transformer Encoder
    # num_heads=2: å…³æ³¨ä¸åŒçš„æ³¢æ®µç»„åˆæ¨¡å¼
    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=0.1)

    # --- Classification Head ---
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(0.2)(x)

    # äºŒåˆ†ç±»è¾“å‡º (Sigmoid): 0=èƒŒæ™¯, 1=PET
    outputs = layers.Dense(1, activation="sigmoid")(x)

    return models.Model(inputs, outputs, name="PET_Transformer_Model")


# ================= 4. å›è°ƒå‡½æ•° =================
class SmartModelCheckpoint(tf.keras.callbacks.Callback):
    def __init__(self, save_dir):
        super(SmartModelCheckpoint, self).__init__()
        self.save_dir = save_dir
        self.best_acc = -float('inf')

    def on_epoch_end(self, epoch, logs=None):
        current_acc = logs.get('val_accuracy')
        if current_acc is not None and current_acc > self.best_acc:
            time_str = datetime.datetime.now().strftime("%Y%m%d-%H%M")
            filename = f"pet_transformer_{time_str}_acc_{current_acc:.4f}.h5"
            save_path = os.path.join(self.save_dir, filename)
            self.model.save(save_path)
            print(f"\nğŸ’¾ [æ–°çºªå½•] ç²¾åº¦: {current_acc:.4f} -> å·²ä¿å­˜: {filename}")
            self.best_acc = current_acc


# ================= 5. ä¸»æµç¨‹ =================
if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨åŠ è½½æ–°ç”Ÿæˆçš„äºŒåˆ†ç±»æ•°æ®é›† (X.npy, y.npy)...")
    x_path = os.path.join(DATA_DIR, "X.npy")
    y_path = os.path.join(DATA_DIR, "y.npy")

    if not os.path.exists(x_path):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {x_path}")
        exit()

    X = np.load(x_path).astype(np.float32)
    y = np.load(y_path).astype(np.float32)

    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæ¯•: {X.shape}")
    print(f"   æ­£æ ·æœ¬(PET): {np.sum(y == 1)} | è´Ÿæ ·æœ¬(BG/CC/PA): {np.sum(y == 0)}")

    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # æ„å»ºæ•°æ®ç®¡é“
    print("âš¡ æ„å»ºé«˜é€Ÿæ•°æ®æµæ°´çº¿...")
    train_ds = create_dataset(X_train, y_train, is_training=True)
    val_ds = create_dataset(X_test, y_test, is_training=False)

    # æ„å»ºå¹¶ç¼–è¯‘æ¨¡å‹
    model = build_transformer_model(input_shape=(X.shape[1],))
    model.summary()

    model.compile(optimizer=optimizers.Adam(1e-4),
                  loss="binary_crossentropy",  # é€‚ç”¨äº 0/1 äºŒåˆ†ç±»
                  metrics=["accuracy"])

    print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ Transformer æ¨¡å‹ (Batch Size={BATCH_SIZE})...")

    # è®­ç»ƒ
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        callbacks=[SmartModelCheckpoint(save_dir=MODEL_SAVE_DIR)]
    )

    print("âœ… è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨å¯¼å‡º ONNX...")

    # å¯¼å‡ºæœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(MODEL_SAVE_DIR, "final_transformer_model.h5")
    model.save(final_path)

    # å¯¼å‡º ONNX (ç”¨äºC++éƒ¨ç½²)
    spec = (tf.TensorSpec((None, X.shape[1]), tf.float32, name="input"),)
    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)

    onnx_path = os.path.join(MODEL_SAVE_DIR, "pet_transformer.onnx")
    with open(onnx_path, "wb") as f:
        f.write(model_proto.SerializeToString())
    print(f"ğŸ† ONNX å¯¼å‡ºæˆåŠŸ: {onnx_path}")