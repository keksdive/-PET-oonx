import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import os
import json


# è¯»å–è‡ªåŠ¨ç”Ÿæˆçš„é…ç½®æ–‡ä»¶
CONFIG_FILE = "best_bands_config.json"

if os.path.exists(CONFIG_FILE):
    with open(CONFIG_FILE, 'r') as f:
        data = json.load(f)
        SELECTED_BANDS = data["selected_bands"]
    print(f"ğŸ¤– [Auto] å·²ä»é…ç½®æ–‡ä»¶åŠ è½½ {len(SELECTED_BANDS)} ä¸ªæ³¢æ®µ")
else:
    # é»˜è®¤å›é€€ï¼ˆå¦‚æœæ²¡æœ‰è·‘ Step 1ï¼‰
    print("âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç¡¬ç¼–ç æ³¢æ®µ")
    SELECTED_BANDS = [19, 39, 62, ...]

# ... (åç»­ä»£ç ä¿æŒä¸å˜ï¼Œç¡®ä¿æ‰€æœ‰ç”¨åˆ° SELECTED_BANDS çš„åœ°æ–¹éƒ½ä½¿ç”¨è¿™ä¸ªå˜é‡)


# å¯ç”¨æ··åˆç²¾åº¦ï¼Œæå‡é€Ÿåº¦ (é’ˆå¯¹ NVIDIA GPU)
from tensorflow.keras import mixed_precision

mixed_precision.set_global_policy('mixed_float16')


# ================= 1. ä¼˜åŒ–åçš„æ¨¡å‹æ¶æ„ (CNN + Transformer) =================
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_spectral_transformer(input_shape):
    inputs = layers.Input(shape=input_shape)
    # (Batch, 30) -> (Batch, 30, 1)
    x = layers.Reshape((input_shape[0], 1))(inputs)

    # --- æ–°å¢: 1D-CNN å±€éƒ¨ç‰¹å¾æå–å±‚ ---
    # æ•æ‰å…‰è°±æ›²çº¿çš„å±€éƒ¨æ–œç‡å’Œæ³¢å³°ç‰¹å¾
    x = layers.Conv1D(filters=32, kernel_size=3, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv1D(filters=64, kernel_size=3, padding="same", activation="relu")(x)

    # --- Transformer ç¼–ç å±‚ ---
    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=0.1)
    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=0.1)

    # å…¨å±€æ± åŒ–ï¼Œæ¯” Flatten æ›´é²æ£’ï¼Œå‡å°‘å‚æ•°é‡æé«˜é€Ÿåº¦
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(0.2)(x)

    # è¾“å‡ºå±‚ï¼šæ³¨æ„æ··åˆç²¾åº¦ä¸‹ï¼Œæœ€åçš„æ¿€æ´»å»ºè®®ç”¨ float32
    outputs = layers.Dense(1, activation="sigmoid", dtype='float32')(x)

    return models.Model(inputs, outputs)


if __name__ == "__main__":
    BASE_DIR = r"I:\Hyperspectral Camera Dataset\Processed_Data"

    # åŠ è½½æ•°æ®é€»è¾‘...
    # X = np.load(...) , y = np.load(...)

    # --- A. æ•°æ®å¢å¼ºï¼šSMOTE (è§£å†³ç±»åˆ«ä¸å¹³è¡¡) ---
    print("â­ æ­£åœ¨æ‰§è¡Œ SMOTE æ•°æ®å¢å¼º...")
    sm = SMOTE(random_state=42)
    X_res, y_res = sm.fit_resample(X, y)

    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)

    # --- B. æ„å»ºå¹¶ç¼–è¯‘æ¨¡å‹ ---
    model = build_spectral_transformer(input_shape=(30,))
    model.compile(
        optimizer=optimizers.Adam(learning_rate=5e-4),  # ç¨å¾®è°ƒé«˜å­¦ä¹ ç‡é…åˆæ··åˆç²¾åº¦
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )

    # æ—©åœä¸å­¦ä¹ ç‡è¡°å‡
    lr_reducer = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒä¼˜åŒ–åçš„æ¨¡å‹...")
    model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=200,
        batch_size=512,  # æ··åˆç²¾åº¦å¯ä»¥ä½¿ç”¨æ›´å¤§çš„ batch sizeï¼Œé€Ÿåº¦é£å¿«
        callbacks=[early_stop, lr_reducer]
    )

    model.save(os.path.join(BASE_DIR, "optimized_spectral_model.1.0.h5"))
    print("âœ… ä¼˜åŒ–åçš„æ¨¡å‹å·²ä¿å­˜ã€‚")