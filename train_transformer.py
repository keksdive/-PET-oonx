import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import os
import json
import glob
import sys

# å¯ç”¨æ··åˆç²¾åº¦ï¼Œæå‡é€Ÿåº¦ (é’ˆå¯¹ NVIDIA GPU)
from tensorflow.keras import mixed_precision

mixed_precision.set_global_policy('mixed_float16')

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
CONFIG_FILE = "best_bands_config.json"
# é¢„å¤„ç†æ•°æ®çš„å­˜æ”¾ç›®å½• (è¯·ç¡®ä¿ save_data.py è¾“å‡ºåˆ°äº†è¿™é‡Œ)
NPY_DIR = r"D:\DRL\DRL1\processed_data"
# æ¨¡å‹ä¿å­˜ç›®å½•
MODEL_SAVE_DIR = r"D:\DRL\DRL1\models"

if not os.path.exists(MODEL_SAVE_DIR):
    os.makedirs(MODEL_SAVE_DIR)

# ================= 1. æ³¢æ®µåŠ è½½é€»è¾‘ =================
if os.path.exists(CONFIG_FILE):
    with open(CONFIG_FILE, 'r') as f:
        data = json.load(f)
        SELECTED_BANDS = data["selected_bands"]
    print(f"ğŸ¤– [Auto] å·²ä»é…ç½®æ–‡ä»¶åŠ è½½ {len(SELECTED_BANDS)} ä¸ªæ³¢æ®µ")
else:
    # é»˜è®¤å›é€€ï¼ˆä¿®å¤äº† ... è¯­æ³•é”™è¯¯ï¼‰
    print("âš ï¸ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç¡¬ç¼–ç æ³¢æ®µ")
    SELECTED_BANDS = [19, 39, 62, 69, 70, 72, 74, 76, 78, 83, 90, 93, 95, 103, 105, 106, 112, 115, 123, 128, 133, 140,
                      143, 150, 160, 172, 174, 180, 187, 197]


# ================= 2. ä¼˜åŒ–åçš„æ¨¡å‹æ¶æ„ (CNN + Transformer) =================
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_spectral_transformer(input_shape):
    inputs = layers.Input(shape=input_shape)
    # (Batch, 30) -> (Batch, 30, 1)
    x = layers.Reshape((input_shape[0], 1))(inputs)

    # --- 1D-CNN å±€éƒ¨ç‰¹å¾æå– ---
    x = layers.Conv1D(filters=32, kernel_size=3, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv1D(filters=64, kernel_size=3, padding="same", activation="relu")(x)

    # --- Transformer ç¼–ç å±‚ ---
    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=0.1)
    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=0.1)

    # å…¨å±€æ± åŒ–
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(0.2)(x)

    # è¾“å‡ºå±‚ (äºŒåˆ†ç±»: 1=PET, 0=éPET/èƒŒæ™¯)
    outputs = layers.Dense(1, activation="sigmoid", dtype='float32')(x)

    return models.Model(inputs, outputs)


if __name__ == "__main__":
    # ================= 3. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† =================
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½ .npy æ•°æ® (è·¯å¾„: {NPY_DIR})...")

    X_list = []
    y_list = []

    # æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æ–‡ä»¶
    data_files = glob.glob(os.path.join(NPY_DIR, "*_data.npy"))

    if not data_files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ {NPY_DIR} ä¸‹æœªæ‰¾åˆ° .npy æ–‡ä»¶ï¼")
        print("   è¯·å…ˆè¿è¡Œ save_data.py è¿›è¡Œæ•°æ®æ ¼å¼è½¬æ¢ã€‚")
        sys.exit(1)

    for d_file in data_files:
        try:
            m_file = d_file.replace("_data.npy", "_mask.npy")
            if not os.path.exists(m_file): continue

            # åŠ è½½å…¨æ³¢æ®µæ•°æ®
            data = np.load(d_file)  # (H, W, Total_Bands)
            mask = np.load(m_file)  # (H, W)

            # åªå–é€‰å®šçš„æ³¢æ®µ
            data_selected = data[:, :, SELECTED_BANDS]

            # --- SNV é¢„å¤„ç† (Paper Optimization) ---
            h, w, c = data_selected.shape
            flat_data = data_selected.reshape(-1, c)

            mean = np.mean(flat_data, axis=1, keepdims=True)
            std = np.std(flat_data, axis=1, keepdims=True)
            std[std == 0] = 1e-6
            flat_data_snv = (flat_data - mean) / std

            flat_mask = mask.reshape(-1)

            # --- å…³é”®ï¼šä¸‰ç±»é‡‡æ ·ç­–ç•¥ ---
            # Label 1: PET (æ­£æ ·æœ¬)
            # Label 2: å¼ºè´Ÿæ ·æœ¬ (PP, PE, CC ç­‰)
            # Label 0: å¼±è´Ÿæ ·æœ¬ (é»‘è‰²èƒŒæ™¯)

            idx_pet = np.where(flat_mask == 1)[0]
            idx_mat = np.where(flat_mask == 2)[0]
            idx_bg = np.where(flat_mask == 0)[0]

            # é‡‡æ ·å¹³è¡¡ (é˜²æ­¢æŸå¼ å›¾èƒŒæ™¯å¤ªå¤šæ·¹æ²¡æ•°æ®)
            # ç­–ç•¥ï¼šä¿è¯ PET å……è¶³ï¼ŒåŒæ—¶å¼•å…¥è¶³å¤Ÿå¤šçš„é PET æè´¨å’Œä¸€éƒ¨åˆ†èƒŒæ™¯

            # 1. å– PET (æœ€å¤š 3000)
            if len(idx_pet) > 3000:
                idx_pet = np.random.choice(idx_pet, 3000, replace=False)

            # 2. å– éPETæè´¨ (æœ€å¤š 2000) -> å®ƒæ˜¯å¼ºå¹²æ‰°é¡¹ï¼Œè¦å¤šå­¦
            if len(idx_mat) > 2000:
                idx_mat = np.random.choice(idx_mat, 2000, replace=False)

            # 3. å– èƒŒæ™¯ (æœ€å¤š 1000) -> å®ƒæ˜¯å¼±å¹²æ‰°é¡¹ï¼Œä½†ä¹Ÿå¾—å­¦ä¸€ç‚¹
            if len(idx_bg) > 1000:
                idx_bg = np.random.choice(idx_bg, 1000, replace=False)

            # æ·»åŠ åˆ°åˆ—è¡¨
            if len(idx_pet) > 0:
                X_list.append(flat_data_snv[idx_pet])
                y_list.append(np.ones(len(idx_pet)))  # Label 1 -> 1 (PET)

            if len(idx_mat) > 0:
                X_list.append(flat_data_snv[idx_mat])
                y_list.append(np.zeros(len(idx_mat)))  # Label 2 -> 0 (éPET)

            if len(idx_bg) > 0:
                X_list.append(flat_data_snv[idx_bg])
                y_list.append(np.zeros(len(idx_bg)))  # Label 0 -> 0 (èƒŒæ™¯)

        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {os.path.basename(d_file)}: {e}")

    # åˆå¹¶æ•°æ®
    if not X_list:
        raise ValueError("âŒ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæ ·æœ¬ï¼è¯·æ£€æŸ¥ save_data.py çš„è¾“å‡ºã€‚")

    X = np.concatenate(X_list, axis=0)
    y = np.concatenate(y_list, axis=0)

    print(f"âœ… æ•°æ®åŠ è½½å®Œæ¯•: æ€»æ ·æœ¬ {len(y)}")
    print(f"   - æ­£æ ·æœ¬ (PET): {np.sum(y == 1)}")
    print(f"   - è´Ÿæ ·æœ¬ (èƒŒæ™¯+æ‚è´¨): {np.sum(y == 0)}")

    # æ£€æŸ¥ç±»åˆ«æ•°
    if len(np.unique(y)) < 2:
        print("âŒ è‡´å‘½é”™è¯¯ï¼šæ•°æ®ä¸­åªåŒ…å« 1 ç§ç±»åˆ«ï¼Œæ— æ³•è®­ç»ƒï¼")
        print("   è¯·ç¡®ä¿ processed_data ä¸­æ—¢åŒ…å« PET æ–‡ä»¶ï¼Œä¹ŸåŒ…å« no_PET æ–‡ä»¶ã€‚")
        sys.exit(1)

    # ================= 4. è®­ç»ƒæµç¨‹ =================

    # --- SMOTE æ•°æ®å¢å¼º ---
    print("â­ æ­£åœ¨æ‰§è¡Œ SMOTE ç±»åˆ«å¹³è¡¡...")
    sm = SMOTE(random_state=42)
    X_res, y_res = sm.fit_resample(X, y)
    print(f"   - å¹³è¡¡åæ ·æœ¬æ•°: {len(y_res)}")

    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)

    # --- æ„å»ºæ¨¡å‹ ---
    model = build_spectral_transformer(input_shape=(len(SELECTED_BANDS),))

    model.compile(
        optimizer=optimizers.Adam(learning_rate=5e-4),
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )

    # --- å›è°ƒå‡½æ•° ---
    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
    lr_reducer = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)
    checkpoint = ModelCheckpoint(
        os.path.join(MODEL_SAVE_DIR, "best_model.h5"),
        monitor='val_accuracy',
        save_best_only=True
    )

    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=200,
        batch_size=512,
        callbacks=[early_stop, lr_reducer, checkpoint]
    )

    # ================= 5. ä¿å­˜ä¸å¯¼å‡º =================
    # ä¿å­˜æœ€ç»ˆ Keras æ¨¡å‹
    final_path = os.path.join(MODEL_SAVE_DIR, "final_transformer_model.h5")
    model.save(final_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {final_path}")

    # å¯¼å‡º ONNX
    import tf2onnx

    onnx_path = os.path.join(MODEL_SAVE_DIR, "pet_classifier.onnx")
    print(f"ğŸ”„ æ­£åœ¨å¯¼å‡º ONNX: {onnx_path} ...")

    spec = (tf.TensorSpec((None, len(SELECTED_BANDS)), tf.float32, name="input"),)
    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)

    with open(onnx_path, "wb") as f:
        f.write(model_proto.SerializeToString())

    print("ğŸ† éƒ¨ç½²æ–‡ä»¶ç”Ÿæˆå®Œæ¯•ï¼")