import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
import json
import numpy as np
from tensorflow.keras.callbacks import ModelCheckpoint
import os
import time
import datetime
import tf2onnx
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix



# ================= 1. ç¡¬ä»¶æ£€æŸ¥ =================
gpus = tf.config.list_physical_devices('GPU')
print(f"\n{'=' * 40}")
if len(gpus) > 0:
    print(f"âœ… æ˜¾å¡å°±ç»ª: {gpus[0].name}")
    try:
        from tensorflow.keras import mixed_precision

        mixed_precision.set_global_policy('mixed_float16')
        print("âš¡ å·²å¯ç”¨æ··åˆç²¾åº¦ (Mixed Precision) åŠ é€Ÿ")
    except:
        pass
else:
    print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†åœ¨ CPU ä¸Šè¿è¡Œ Transformer (é€Ÿåº¦è¾ƒæ…¢)")
print(f"{'=' * 40}\n")

# ================= ğŸ”§ è·¯å¾„é…ç½® =================
DATA_DIR = r"D:\Processed_Result\json-procession-result"
MODEL_SAVE_DIR = r"D:\Processed_Result\67w-38w\models63w"
if not os.path.exists(MODEL_SAVE_DIR): os.makedirs(MODEL_SAVE_DIR)

# âš¡ å‚æ•°é…ç½®
BATCH_SIZE = 4096
EPOCHS = 300

# å®šä¹‰ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œç”¨äºåœ¨è®­ç»ƒæ—¶ç”Ÿæˆæ··æ·†çŸ©é˜µ
class ConfusionMatrixLogger(tf.keras.callbacks.Callback):
    def __init__(self, X_val, y_val, class_names, interval=1, save_dir="logs/cm_plots"):
        super(ConfusionMatrixLogger, self).__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.class_names = class_names
        self.interval = interval
        self.save_dir = save_dir
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

    def on_epoch_end(self, epoch, logs=None):
        # æ¯éš” interval ä¸ª epoch æ‰§è¡Œä¸€æ¬¡ï¼ˆé¿å…æ¯ä¸ª epoch éƒ½ç”»å›¾å¤ªæ…¢ï¼‰
        if (epoch + 1) % self.interval == 0:
            print(f"\næ­£åœ¨è®¡ç®—ç¬¬ {epoch + 1} è½®çš„æ··æ·†çŸ©é˜µ...")

            # 1. é¢„æµ‹éªŒè¯é›†
            y_pred_probs = self.model.predict(self.X_val, verbose=0)
            y_pred = np.argmax(y_pred_probs, axis=1)

            # 2. å¤„ç†çœŸå®æ ‡ç­¾ï¼ˆå¦‚æœæ˜¯ One-hot ç¼–ç ï¼Œè½¬ä¸ºç´¢å¼•ï¼‰
            if self.y_val.ndim > 1 and self.y_val.shape[1] > 1:
                y_true = np.argmax(self.y_val, axis=1)
            else:
                y_true = self.y_val

            # 3. è®¡ç®—æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_true, y_pred)

            # 4. ç»˜åˆ¶å¹¶ä¿å­˜å›¾ç‰‡
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=self.class_names,
                        yticklabels=self.class_names)
            plt.title(f'Confusion Matrix - Epoch {epoch + 1}')
            plt.ylabel('True Label (çœŸå®)')
            plt.xlabel('Predicted Label (é¢„æµ‹)')

            save_path = os.path.join(self.save_dir, f'cm_epoch_{epoch + 1}.png')
            plt.savefig(save_path)
            plt.close()  # å…³é—­ç”»å¸ƒï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
            print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {save_path}")



# ================= 2. æ•°æ®ç®¡é“ =================
def create_dataset(X, y, is_training=True):
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    if is_training:
        dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.cache()
    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)
    return dataset


# ================= 3. Transformer æ¨¡å‹å®šä¹‰ (å¼ºåˆ¶ä½¿ç”¨) =================
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # 1. Normalization & Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # 2. Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_transformer_model(input_shape):
    """
    æ„å»º Hybrid CNN-Transformer æ¨¡å‹
    ç»“åˆ CNN çš„å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›å’Œ Transformer çš„å…¨å±€åºåˆ—å»ºæ¨¡èƒ½åŠ›
    """
    inputs = layers.Input(shape=input_shape)

    # å¢åŠ ä¸€ä¸ªç»´åº¦ä»¥é€‚é… Conv1D: (Batch, Bands) -> (Batch, Bands, 1)
    x = layers.Reshape((input_shape[0], 1))(inputs)

    # --- Feature Extraction (CNN) ---
    # å…ˆç”¨ CNN æå–æ³¢è°±çš„å±€éƒ¨ç‰¹å¾ï¼ˆæ³¢å³°/æ³¢è°·çš„æ–œç‡ç­‰ï¼‰
    x = layers.Conv1D(32, 5, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling1D(2)(x)

    x = layers.Conv1D(64, 3, padding="same", activation="relu")(x)
    x = layers.MaxPooling1D(2)(x)

    # --- Sequence Modeling (Transformer) ---
    # å¼ºåˆ¶ä½¿ç”¨ Transformer Encoder
    # num_heads=2: å…³æ³¨ä¸åŒçš„æ³¢æ®µç»„åˆæ¨¡å¼
    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=0.1)

    # --- Classification Head ---
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(0.2)(x)

    # ä¿®æ”¹ä¸º (ä¸‰åˆ†ç±»: 0=èƒŒæ™¯, 1=PET, 2=PA)
    # è¿™é‡Œçš„ 3 æ˜¯æ‚¨çš„æ€»ç±»åˆ«æ•°ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    outputs = layers.Dense(3, activation="softmax")(x)

    return models.Model(inputs, outputs, name="PET_Transformer_Model")


# ================= 4. æ™ºèƒ½ä¿å­˜å›è°ƒå‡½æ•° (ä¿®æ”¹ç‰ˆ) =================
class SmartModelCheckpoint(tf.keras.callbacks.Callback):
    def __init__(self, save_dir):
        super(SmartModelCheckpoint, self).__init__()
        self.save_dir = save_dir
        # è®°å½•ä¸Šä¸€æ¬¡å› ç²¾åº¦æå‡è€Œä¿å­˜æ—¶çš„ç²¾åº¦ï¼Œåˆå§‹åŒ–ä¸º0
        self.last_milestone_acc = 0.0

    def on_epoch_end(self, epoch, logs=None):
        current_acc = logs.get('val_accuracy')
        if current_acc is None:
            return

        should_save = False
        save_reason = ""

        # --- ç­–ç•¥ 1: æ¯10è½®ä¿å­˜ä¸€æ¬¡ ---
        if (epoch + 1) % 10 == 0:
            should_save = True
            save_reason = f"Epoch {epoch + 1}"

        # --- ç­–ç•¥ 2: ç²¾åº¦é˜¶æ¢¯æå‡ä¿å­˜ ---
        # åˆ¤å®šå½“å‰é˜¶æ®µçš„æå‡é˜ˆå€¼
        if self.last_milestone_acc >= 0.9:
            # ç²¾åº¦è¾¾åˆ°0.9ä¹‹åï¼Œæ¯0.05ä¿å­˜ä¸€æ¬¡
            threshold = 0.05
        else:
            # ç²¾åº¦æœªåˆ°0.9ï¼Œæ¯0.01ä¿å­˜ä¸€æ¬¡
            threshold = 0.01

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æå‡æ¡ä»¶
        if current_acc >= (self.last_milestone_acc + threshold):
            should_save = True
            # æ›´æ–°é‡Œç¨‹ç¢‘åŸºå‡†ï¼ˆåªæœ‰è§¦å‘äº†ç²¾åº¦ä¿å­˜æ‰æ›´æ–°è¿™ä¸ªåŸºå‡†ï¼‰
            self.last_milestone_acc = current_acc
            save_reason = f"Acc Improved (+{threshold})"

        # --- æ‰§è¡Œä¿å­˜ ---
        if should_save:
            # æ ¼å¼ï¼šä¿å­˜æ—¶é—´-å½“å‰ç²¾åº¦-models.h5
            time_str = datetime.datetime.now().strftime("%Y%m%d-%H%M")
            # ä¿æŒæ–‡ä»¶åä¸­ç²¾åº¦çš„æ ¼å¼æ•´æ´ï¼Œä¾‹å¦‚ 0.9500
            filename = f"{time_str}-{current_acc:.4f}-models.h5"
            save_path = os.path.join(self.save_dir, filename)

            self.model.save(save_path)
            print(f"\nğŸ’¾ [è‡ªåŠ¨ä¿å­˜] è§¦å‘: {save_reason} | å½“å‰ç²¾åº¦: {current_acc:.4f} -> å·²ä¿å­˜: {filename}")


# ================= 5. ä¸»æµç¨‹ =================
if __name__ == "__main__":
    # 1. åŠ è½½å…¨é‡æ•°æ®
    print("ğŸš€ æ­£åœ¨åŠ è½½æ–°ç”Ÿæˆçš„äºŒåˆ†ç±»æ•°æ®é›† (X.npy, y.npy)...")
    x_path = os.path.join(DATA_DIR, "X.npy")
    y_path = os.path.join(DATA_DIR, "y.npy")

    if not os.path.exists(x_path):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {x_path}")
        exit()

    X = np.load(x_path).astype(np.float32)
    y = np.load(y_path).astype(np.float32)

    # 2. åŠ è½½æ³¢æ®µé…ç½®æ–‡ä»¶
    config_path = "best_bands_config.json"  # ç¡®ä¿æ­¤æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ³¢æ®µé…ç½®æ–‡ä»¶: {config_path}ï¼Œè¯·å…ˆè¿è¡Œ main.py")

    with open(config_path, 'r') as f:
        config = json.load(f)
        selected_bands = config["selected_bands"]

    print(f"ğŸ¤– [Auto] å·²åŠ è½½ {len(selected_bands)} ä¸ªç‰¹å¾æ³¢æ®µé…ç½®ã€‚")
    print(f"   -> åŸå§‹ç»´åº¦: {X.shape}")

    # 3. æ‰§è¡Œç‰¹å¾åˆ‡ç‰‡ (Slicing)
    # åªä¿ç•™é€‰ä¸­çš„æ³¢æ®µï¼ŒæŠ›å¼ƒå…¶ä»–æ³¢æ®µ
    X = X[:, selected_bands]
    print(f"   -> åˆ‡ç‰‡åç»´åº¦: {X.shape} (ç”¨äºè®­ç»ƒ)")

    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæ¯•: {X.shape}")
    print(f"   æ­£æ ·æœ¬(PET): {np.sum(y == 1)} | è´Ÿæ ·æœ¬(BG/CC/PA): {np.sum(y == 0)}")

    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # æ„å»ºæ•°æ®ç®¡é“
    print("âš¡ æ„å»ºé«˜é€Ÿæ•°æ®æµæ°´çº¿...")
    train_ds = create_dataset(X_train, y_train, is_training=True)
    val_ds = create_dataset(X_test, y_test, is_training=False)

    # æ„å»ºå¹¶ç¼–è¯‘æ¨¡å‹
    model = build_transformer_model(input_shape=(X.shape[1],))
    model.summary()

    model.compile(optimizer=optimizers.Adam(1e-4),
                  # åŸä»£ç : loss="binary_crossentropy",
                  # ä¿®æ”¹ä¸º: é€‚ç”¨äºæ•´æ•°æ ‡ç­¾(0,1,2)çš„å¤šåˆ†ç±»æŸå¤±
                  loss="sparse_categorical_crossentropy",
                  metrics=["accuracy"])

    print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ Transformer æ¨¡å‹ (Batch Size={BATCH_SIZE})...")

    # ç­–ç•¥ï¼šé™ä½èƒŒæ™¯æƒé‡(0.1)ï¼Œä¿æŒPETæ ‡å‡†(1.0)ï¼Œç‹ ç‹ æƒ©ç½šPAè¯¯åˆ¤(5.0)
    # è¯·ç¡®ä¿æ‚¨çš„ y_train ä¸­åŒ…å«å¯¹åº”çš„ç±»åˆ« ID (0, 1, 2)
    # 0: èƒŒæ™¯ (å®¹æ˜“åŒºåˆ†ï¼Œæƒé‡è°ƒä½ï¼Œè®©æ¨¡å‹åˆ«å¤ªå…³æ³¨å®ƒ)
    # 1: PET (æ­£æ ·æœ¬ï¼Œæ ‡å‡†æƒé‡)
    # 2: PA/å°¼é¾™ (å›°éš¾è´Ÿæ ·æœ¬ï¼Œæƒé‡è°ƒé«˜ï¼Œå¼ºè¿«æ¨¡å‹å­¦ä¼šåŒºåˆ†å®ƒ)
    class_weights = {
        0: 0.1,
        1: 1.0,
        2: 5.0
    }
    # ================= æƒé‡é…ç½® (æ ¸å¿ƒä¿®æ”¹) =================
    print(f"âš–ï¸ å·²å¯ç”¨ç±»åˆ«åŠ æƒç­–ç•¥: {class_weights}")
    # ===============================================
    # è®­ç»ƒ
    # ä¿®æ”¹ model.fit

    # å®ä¾‹åŒ–å›è°ƒå‡½æ•°
    # interval=5 è¡¨ç¤ºæ¯è®­ç»ƒ 5 è½®ç”»ä¸€å¼ å›¾ï¼Œé¿å…æ‹–æ…¢é€Ÿåº¦
    cm_callback = ConfusionMatrixLogger(X_test, y_test, class_names, interval=5)
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_test, y_test),
        # â–¼â–¼â–¼ æŠŠå›è°ƒå‡½æ•°åŠ åœ¨è¿™é‡Œ â–¼â–¼â–¼
        callbacks=[cm_callback]
    )

    print("âœ… è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨å¯¼å‡º ONNX...")

    # å¯¼å‡ºæœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(MODEL_SAVE_DIR, "final_transformer_model.h5")
    model.save(final_path)

    # å¯¼å‡º ONNX (ç”¨äºC++éƒ¨ç½²)
    spec = (tf.TensorSpec((None, X.shape[1]), tf.float32, name="input"),)
    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)

    onnx_path = os.path.join(MODEL_SAVE_DIR, "pet_transformer1.0.1.onnx")
    with open(onnx_path, "wb") as f:
        f.write(model_proto.SerializeToString())
    print(f"ğŸ† ONNX å¯¼å‡ºæˆåŠŸ: {onnx_path}")