import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
import json
import numpy as np
from tensorflow.keras.callbacks import ModelCheckpoint
import os
import time
import datetime
import tf2onnx

# ================= 1. ç¡¬ä»¶æ£€æŸ¥ =================
gpus = tf.config.list_physical_devices('GPU')
print(f"\n{'=' * 40}")
if len(gpus) > 0:
    print(f"âœ… æ˜¾å¡å°±ç»ª: {gpus[0].name}")
    try:
        from tensorflow.keras import mixed_precision

        mixed_precision.set_global_policy('mixed_float16')
        print("âš¡ å·²å¯ç”¨æ··åˆç²¾åº¦ (Mixed Precision) åŠ é€Ÿ")
    except:
        pass
else:
    print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†åœ¨ CPU ä¸Šè¿è¡Œ Transformer (é€Ÿåº¦è¾ƒæ…¢)")
print(f"{'=' * 40}\n")

# ================= ğŸ”§ è·¯å¾„é…ç½® =================
DATA_DIR = r"E:\SPEDATA\NP_new1.0.2"
MODEL_SAVE_DIR = r"D:\DRL\DRL1\models"
if not os.path.exists(MODEL_SAVE_DIR): os.makedirs(MODEL_SAVE_DIR)

# âš¡ å‚æ•°é…ç½®
BATCH_SIZE = 2048
EPOCHS = 100


# ================= 2. æ•°æ®ç®¡é“ =================
def create_dataset(X, y, is_training=True):
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    if is_training:
        dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.cache()
    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)
    return dataset


# ================= 3. Transformer æ¨¡å‹å®šä¹‰ (å¼ºåˆ¶ä½¿ç”¨) =================
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # 1. Normalization & Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # 2. Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_transformer_model(input_shape):
    """
    æ„å»º Hybrid CNN-Transformer æ¨¡å‹
    ç»“åˆ CNN çš„å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›å’Œ Transformer çš„å…¨å±€åºåˆ—å»ºæ¨¡èƒ½åŠ›
    """
    inputs = layers.Input(shape=input_shape)

    # å¢åŠ ä¸€ä¸ªç»´åº¦ä»¥é€‚é… Conv1D: (Batch, Bands) -> (Batch, Bands, 1)
    x = layers.Reshape((input_shape[0], 1))(inputs)

    # --- Feature Extraction (CNN) ---
    # å…ˆç”¨ CNN æå–æ³¢è°±çš„å±€éƒ¨ç‰¹å¾ï¼ˆæ³¢å³°/æ³¢è°·çš„æ–œç‡ç­‰ï¼‰
    x = layers.Conv1D(32, 5, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling1D(2)(x)

    x = layers.Conv1D(64, 3, padding="same", activation="relu")(x)
    x = layers.MaxPooling1D(2)(x)

    # --- Sequence Modeling (Transformer) ---
    # å¼ºåˆ¶ä½¿ç”¨ Transformer Encoder
    # num_heads=2: å…³æ³¨ä¸åŒçš„æ³¢æ®µç»„åˆæ¨¡å¼
    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=0.1)

    # --- Classification Head ---
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(0.2)(x)

    # äºŒåˆ†ç±»è¾“å‡º (Sigmoid): 0=èƒŒæ™¯, 1=PET
    outputs = layers.Dense(1, activation="sigmoid")(x)

    return models.Model(inputs, outputs, name="PET_Transformer_Model")


# ================= 4. æ™ºèƒ½ä¿å­˜å›è°ƒå‡½æ•° (ä¿®æ”¹ç‰ˆ) =================
class SmartModelCheckpoint(tf.keras.callbacks.Callback):
    def __init__(self, save_dir):
        super(SmartModelCheckpoint, self).__init__()
        self.save_dir = save_dir
        # è®°å½•ä¸Šä¸€æ¬¡å› ç²¾åº¦æå‡è€Œä¿å­˜æ—¶çš„ç²¾åº¦ï¼Œåˆå§‹åŒ–ä¸º0
        self.last_milestone_acc = 0.0

    def on_epoch_end(self, epoch, logs=None):
        current_acc = logs.get('val_accuracy')
        if current_acc is None:
            return

        should_save = False
        save_reason = ""

        # --- ç­–ç•¥ 1: æ¯10è½®ä¿å­˜ä¸€æ¬¡ ---
        if (epoch + 1) % 10 == 0:
            should_save = True
            save_reason = f"Epoch {epoch + 1}"

        # --- ç­–ç•¥ 2: ç²¾åº¦é˜¶æ¢¯æå‡ä¿å­˜ ---
        # åˆ¤å®šå½“å‰é˜¶æ®µçš„æå‡é˜ˆå€¼
        if self.last_milestone_acc >= 0.9:
            # ç²¾åº¦è¾¾åˆ°0.9ä¹‹åï¼Œæ¯0.05ä¿å­˜ä¸€æ¬¡
            threshold = 0.05
        else:
            # ç²¾åº¦æœªåˆ°0.9ï¼Œæ¯0.1ä¿å­˜ä¸€æ¬¡
            threshold = 0.1

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æå‡æ¡ä»¶
        if current_acc >= (self.last_milestone_acc + threshold):
            should_save = True
            # æ›´æ–°é‡Œç¨‹ç¢‘åŸºå‡†ï¼ˆåªæœ‰è§¦å‘äº†ç²¾åº¦ä¿å­˜æ‰æ›´æ–°è¿™ä¸ªåŸºå‡†ï¼‰
            self.last_milestone_acc = current_acc
            save_reason = f"Acc Improved (+{threshold})"

        # --- æ‰§è¡Œä¿å­˜ ---
        if should_save:
            # æ ¼å¼ï¼šä¿å­˜æ—¶é—´-å½“å‰ç²¾åº¦-models.h5
            time_str = datetime.datetime.now().strftime("%Y%m%d-%H%M")
            # ä¿æŒæ–‡ä»¶åä¸­ç²¾åº¦çš„æ ¼å¼æ•´æ´ï¼Œä¾‹å¦‚ 0.9500
            filename = f"{time_str}-{current_acc:.4f}-models.h5"
            save_path = os.path.join(self.save_dir, filename)

            self.model.save(save_path)
            print(f"\nğŸ’¾ [è‡ªåŠ¨ä¿å­˜] è§¦å‘: {save_reason} | å½“å‰ç²¾åº¦: {current_acc:.4f} -> å·²ä¿å­˜: {filename}")


# ================= 5. ä¸»æµç¨‹ =================
if __name__ == "__main__":
    # 1. åŠ è½½å…¨é‡æ•°æ®
    print("ğŸš€ æ­£åœ¨åŠ è½½æ–°ç”Ÿæˆçš„äºŒåˆ†ç±»æ•°æ®é›† (X.npy, y.npy)...")
    x_path = os.path.join(DATA_DIR, "X.npy")
    y_path = os.path.join(DATA_DIR, "y.npy")

    if not os.path.exists(x_path):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {x_path}")
        exit()

    X = np.load(x_path).astype(np.float32)
    y = np.load(y_path).astype(np.float32)

    # 2. åŠ è½½æ³¢æ®µé…ç½®æ–‡ä»¶
    config_path = "best_bands_config.json"  # ç¡®ä¿æ­¤æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ³¢æ®µé…ç½®æ–‡ä»¶: {config_path}ï¼Œè¯·å…ˆè¿è¡Œ main.py")

    with open(config_path, 'r') as f:
        config = json.load(f)
        selected_bands = config["selected_bands"]

    print(f"ğŸ¤– [Auto] å·²åŠ è½½ {len(selected_bands)} ä¸ªç‰¹å¾æ³¢æ®µé…ç½®ã€‚")
    print(f"   -> åŸå§‹ç»´åº¦: {X.shape}")

    # 3. æ‰§è¡Œç‰¹å¾åˆ‡ç‰‡ (Slicing)
    # åªä¿ç•™é€‰ä¸­çš„æ³¢æ®µï¼ŒæŠ›å¼ƒå…¶ä»–æ³¢æ®µ
    X = X[:, selected_bands]
    print(f"   -> åˆ‡ç‰‡åç»´åº¦: {X.shape} (ç”¨äºè®­ç»ƒ)")

    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæ¯•: {X.shape}")
    print(f"   æ­£æ ·æœ¬(PET): {np.sum(y == 1)} | è´Ÿæ ·æœ¬(BG/CC/PA): {np.sum(y == 0)}")

    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # æ„å»ºæ•°æ®ç®¡é“
    print("âš¡ æ„å»ºé«˜é€Ÿæ•°æ®æµæ°´çº¿...")
    train_ds = create_dataset(X_train, y_train, is_training=True)
    val_ds = create_dataset(X_test, y_test, is_training=False)

    # æ„å»ºå¹¶ç¼–è¯‘æ¨¡å‹
    model = build_transformer_model(input_shape=(X.shape[1],))
    model.summary()

    model.compile(optimizer=optimizers.Adam(1e-4),
                  loss="binary_crossentropy",  # é€‚ç”¨äº 0/1 äºŒåˆ†ç±»
                  metrics=["accuracy"])

    print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ Transformer æ¨¡å‹ (Batch Size={BATCH_SIZE})...")

    # è®­ç»ƒ
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        callbacks=[SmartModelCheckpoint(save_dir=MODEL_SAVE_DIR)]
    )

    print("âœ… è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨å¯¼å‡º ONNX...")

    # å¯¼å‡ºæœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(MODEL_SAVE_DIR, "final_transformer_model.h5")
    model.save(final_path)

    # å¯¼å‡º ONNX (ç”¨äºC++éƒ¨ç½²)
    spec = (tf.TensorSpec((None, X.shape[1]), tf.float32, name="input"),)
    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)

    onnx_path = os.path.join(MODEL_SAVE_DIR, "pet_transformer.onnx")
    with open(onnx_path, "wb") as f:
        f.write(model_proto.SerializeToString())
    print(f"ğŸ† ONNX å¯¼å‡ºæˆåŠŸ: {onnx_path}")